Building Function:
def build_branched_model(input_shapes):
    def create_branch(input_shape, branch_id):
        input_layer = Input(shape=input_shape, name=f'input{branch_id}')
        x = Conv1D(filters=64*FILTN, kernel_size=30, strides=10, activation='relu', padding='same', name=f'conv1d_{branch_id}_1')(input_layer)
        x = MaxPooling1D(pool_size=2)(x)
        x = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name=f'conv1d_{branch_id}_2')(x)
        x = Dropout(rate=0.25, name=f'dropout_{branch_id}_1')(x)
        x = Conv1D(filters=256*FILTN, kernel_size=2, strides=1, activation='relu', name=f'conv1d_{branch_id}_3')(x)
        x = GlobalMaxPooling1D(name=f'gap1d_{branch_id}_1')(x)
        return input_layer, x

    inputs = []
    branches = []
    
    for i, input_shape in enumerate(input_shapes, 1):
        input_layer, branch_output = create_branch(input_shape, i)
        inputs.append(input_layer)
        branches.append(branch_output)
    
    merged = concatenate(branches, name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='sigmoid', name='output')(dense)
    
    model = Model(inputs=inputs, outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 2]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 0]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_7"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 800, 1)    │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 800, 1)    │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 800, 2)    │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 800, 2)    │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 80, 64)    │      1,984 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 80, 64)    │      1,984 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 80, 64)    │      3,904 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 80, 64)    │      3,904 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_4     │ (None, 40, 64)    │          0 │ conv1d_1_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_5     │ (None, 40, 64)    │          0 │ conv1d_2_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_6     │ (None, 40, 64)    │          0 │ conv1d_3_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_7     │ (None, 40, 64)    │          0 │ conv1d_4_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 19, 128)   │     32,896 │ max_pooling1d_4[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 19, 128)   │     32,896 │ max_pooling1d_5[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 19, 128)   │     32,896 │ max_pooling1d_6[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 19, 128)   │     32,896 │ max_pooling1d_7[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 19, 128)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 19, 128)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 19, 128)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_4_1         │ (None, 19, 128)   │          0 │ conv1d_4_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 18, 256)   │     65,792 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 18, 256)   │     65,792 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 18, 256)   │     65,792 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_3 (Conv1D) │ (None, 18, 256)   │     65,792 │ dropout_4_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0],  │
│                     │                   │            │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     65,600 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 1)         │         65 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,416,581 (5.40 MB)
 Trainable params: 472,193 (1.80 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 944,388 (3.60 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f378073ee00>
Loss Function: binary_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.04916524142026901
Test val_loss: 0.24816623330116272
Train accuracy: 0.9783845543861389
Accuracy Score: 0.9488054607508533
F1 Score: 0.9523809523809523
Classification Report:
               precision    recall  f1-score   support

         0.0       0.96      0.93      0.94       137
         1.0       0.94      0.96      0.95       156

    accuracy                           0.95       293
   macro avg       0.95      0.95      0.95       293
weighted avg       0.95      0.95      0.95       293

Training History:
accuracy: [0.5824800729751587, 0.7133105993270874, 0.7292377948760986, 0.7383390069007874, 0.7713310718536377, 0.7952218651771545, 0.8122866749763489, 0.8248009085655212, 0.8304892182350159, 0.8350397944450378, 0.8452787399291992, 0.831626832485199, 0.8623435497283936, 0.8691695332527161, 0.8794084191322327, 0.8782707452774048, 0.8805460929870605, 0.8953356146812439, 0.9055745005607605, 0.9021615386009216, 0.9010238647460938, 0.939704179763794, 0.9431172013282776, 0.9112628102302551, 0.9613196849822998, 0.9431172013282776, 0.9465301632881165, 0.9488054513931274, 0.9601820111274719, 0.9499431252479553, 0.9579067230224609, 0.9590443968772888, 0.9601820111274719, 0.9704209566116333, 0.9670079350471497, 0.9658703207969666, 0.9704209566116333, 0.9874857664108276, 0.9749715328216553, 0.9670079350471497, 0.9658703207969666, 0.9761092066764832, 0.9783845543861389]
loss: [0.6736003160476685, 0.574230432510376, 0.5411924123764038, 0.505142331123352, 0.45417964458465576, 0.39822572469711304, 0.3933570683002472, 0.37867265939712524, 0.35869327187538147, 0.33466267585754395, 0.3500608503818512, 0.3508632481098175, 0.29525232315063477, 0.2737569510936737, 0.2755107581615448, 0.2930128276348114, 0.27381590008735657, 0.2524927854537964, 0.23117183148860931, 0.20866091549396515, 0.2169777899980545, 0.17213048040866852, 0.1520935446023941, 0.21331563591957092, 0.13159187138080597, 0.14633148908615112, 0.1500472128391266, 0.13108953833580017, 0.10760682821273804, 0.1141492947936058, 0.11463938653469086, 0.10021425038576126, 0.10335882753133774, 0.08631982654333115, 0.09497850388288498, 0.08449416607618332, 0.08083280175924301, 0.04869424179196358, 0.062236230820417404, 0.07948290556669235, 0.09451418370008469, 0.0656379908323288, 0.04916524142026901]
val_accuracy: [0.6962457299232483, 0.7508532404899597, 0.703071653842926, 0.6996586918830872, 0.7610921263694763, 0.7986348271369934, 0.8156996369361877, 0.8327645063400269, 0.7849829196929932, 0.8430033922195435, 0.7269624471664429, 0.849829375743866, 0.8327645063400269, 0.8464163541793823, 0.8532423377037048, 0.8873720169067383, 0.9180887341499329, 0.849829375743866, 0.8839590549468994, 0.8873720169067383, 0.9078498482704163, 0.9044368863105774, 0.8668941855430603, 0.9078498482704163, 0.914675772190094, 0.8976109027862549, 0.9215016961097717, 0.914675772190094, 0.9283276200294495, 0.9249146580696106, 0.9419795274734497, 0.9283276200294495, 0.894197940826416, 0.9010238647460938, 0.935153603553772, 0.9180887341499329, 0.9180887341499329, 0.935153603553772, 0.9317406415939331, 0.914675772190094, 0.9249146580696106, 0.9180887341499329, 0.914675772190094]
val_loss: [0.5988359451293945, 0.5366078615188599, 0.5753288269042969, 0.5713955760002136, 0.47938281297683716, 0.42022523283958435, 0.3603771924972534, 0.35583096742630005, 0.4204157590866089, 0.3321317136287689, 0.5233857035636902, 0.3253312110900879, 0.34697961807250977, 0.3375428318977356, 0.32714003324508667, 0.2694985270500183, 0.25942692160606384, 0.30877429246902466, 0.2521648705005646, 0.2594713568687439, 0.24884819984436035, 0.2242482602596283, 0.3367217183113098, 0.2335149347782135, 0.21523229777812958, 0.22862134873867035, 0.19873671233654022, 0.2149190455675125, 0.1818237155675888, 0.1973358541727066, 0.18327222764492035, 0.18145254254341125, 0.29820480942726135, 0.26692232489585876, 0.16362571716308594, 0.21580442786216736, 0.24301132559776306, 0.1848258674144745, 0.1983199566602707, 0.19473165273666382, 0.2003483921289444, 0.2466648817062378, 0.24816623330116272]

Confusion Matrix:
[[128   9]
 [  6 150]]

################################################################################################ 

