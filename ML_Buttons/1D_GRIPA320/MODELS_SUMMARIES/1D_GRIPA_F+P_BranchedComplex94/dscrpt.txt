def build_SeparatedEASYmodel(F_input_shape, P_input_shape):
    force_input = Input(shape=F_input_shape, name='force_input')
    Fbranch1 = Sequential()
    Fbranch1.add(Conv1D(64, kernel_size=40, strides=10, activation='relu', input_shape=F_input_shape))
    Fbranch1.add(MaxPooling1D(pool_size=2))
    Fbranch1.add(Conv1D(128, kernel_size=20, strides=2, activation='relu'))
    Fbranch1.add(MaxPooling1D(pool_size=2))
    Fbranch1.add(Conv1D(256, kernel_size=4, activation='relu'))
    Fbranch1.add(GlobalMaxPooling1D())
    Fbranch1.add(Dense(128, activation='relu'))
    Fbranch1.add(Dropout(0.2))
    Fbranch1.add(Dense(64, activation='relu'))
    outF1 = Fbranch1(force_input)

    Fbranch2 = Sequential()
    Fbranch2.add(Conv1D(64, kernel_size=2,  activation='relu', input_shape=F_input_shape))
    Fbranch2.add(MaxPooling1D(pool_size=2))
    Fbranch2.add(Conv1D(64, kernel_size=8, strides=2, activation='relu'))
    Fbranch2.add(MaxPooling1D(pool_size=2))
    Fbranch2.add(Conv1D(128, kernel_size=4, activation='relu'))
    Fbranch2.add(Conv1D(64, kernel_size=2, activation='relu'))
    Fbranch2.add(GlobalMaxPooling1D())
    Fbranch2.add(Dropout(0.1))
    Fbranch2.add(Dense(64, activation='relu'))
    outF2 = Fbranch2(force_input)
    
    pose_input = Input(shape=F_input_shape, name='pose_input')
    Pbranch = Sequential()
    Pbranch.add(Conv1D(64, kernel_size=40, strides=10, activation='relu', input_shape=P_input_shape))
    Pbranch.add(MaxPooling1D(pool_size=2))
    Pbranch.add(Conv1D(64, kernel_size=10, activation='relu'))
    Pbranch.add(MaxPooling1D(pool_size=2))
    Pbranch.add(Flatten())
    Pbranch.add(Dense(16, activation='relu'))
    outP = Pbranch(pose_input)
    
    merged = concatenate([outF1, outF2, outP])
    merged = Dropout(0.1)(merged)
    merged = Dense(64, activation='relu')(merged)
    merged = Dense(1, activation='sigmoid')(merged)
    separated_model = Model(inputs=[force_input, force_input, pose_input], outputs=merged)
    
    return separated_model

Model: "functional_37"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ force_input         │ (None, 800, 1)    │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ pose_input          │ (None, 800, 1)    │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_3        │ (None, 64)        │    339,072 │ force_input[0][0] │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_4        │ (None, 64)        │     86,528 │ force_input[0][0] │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_5        │ (None, 16)        │     63,120 │ pose_input[0][0]  │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate         │ (None, 144)       │          0 │ sequential_3[0][… │
│ (Concatenate)       │                   │            │ sequential_4[0][… │
│                     │                   │            │ sequential_5[0][… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2 (Dropout) │ (None, 144)       │          0 │ concatenate[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_4 (Dense)     │ (None, 64)        │      9,280 │ dropout_2[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_5 (Dense)     │ (None, 1)         │         65 │ dense_4[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,494,197 (5.70 MB)
 Trainable params: 498,065 (1.90 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 996,132 (3.80 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adamw.AdamW object at 0x7be2646a4610>
Loss Function: binary_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adamw/learning_rate>

Train loss: 0.20409604907035828
Test val_loss: 0.2177533209323883
Train accuracy: 0.9224599003791809
Accuracy Score: 0.944
F1 Score: 0.9416666666666667
Classification Report:
               precision    recall  f1-score   support

         0.0       0.94      0.95      0.95       129
         1.0       0.95      0.93      0.94       121

    accuracy                           0.94       250
   macro avg       0.94      0.94      0.94       250
weighted avg       0.94      0.94      0.94       250

Training History:
accuracy: [0.5494652390480042, 0.6016042828559875, 0.6657754182815552, 0.6911764740943909, 0.7032085657119751, 0.7139037251472473, 0.7058823704719543, 0.73128342628479, 0.7606951594352722, 0.7967914342880249, 0.8128342032432556, 0.8288770318031311, 0.8382353186607361, 0.8516042828559875, 0.893048107624054, 0.8917112350463867, 0.8997326493263245, 0.9224599003791809]
loss: [0.693331241607666, 0.6624115109443665, 0.6063567996025085, 0.569698691368103, 0.5321111679077148, 0.5174651741981506, 0.5274031758308411, 0.5083128809928894, 0.4632405638694763, 0.4021762013435364, 0.390438437461853, 0.33925387263298035, 0.3245425224304199, 0.32389143109321594, 0.2537820637226105, 0.23353303968906403, 0.22236086428165436, 0.20409604907035828]
val_accuracy: [0.5839999914169312, 0.6399999856948853, 0.6159999966621399, 0.7239999771118164, 0.7160000205039978, 0.7200000286102295, 0.7559999823570251, 0.7319999933242798, 0.7639999985694885, 0.7680000066757202, 0.8199999928474426, 0.8479999899864197, 0.7960000038146973, 0.8840000033378601, 0.8519999980926514, 0.9120000004768372, 0.9079999923706055, 0.9279999732971191]
val_loss: [0.6700085401535034, 0.6389289498329163, 0.6191144585609436, 0.543716549873352, 0.48991522192955017, 0.5363276600837708, 0.48240774869918823, 0.4699456989765167, 0.4228709042072296, 0.40818163752555847, 0.34493300318717957, 0.3198200464248657, 0.33452221751213074, 0.2990175783634186, 0.2698557376861572, 0.2556128203868866, 0.26273399591445923, 0.2177533209323883]

################################################################################################ 

