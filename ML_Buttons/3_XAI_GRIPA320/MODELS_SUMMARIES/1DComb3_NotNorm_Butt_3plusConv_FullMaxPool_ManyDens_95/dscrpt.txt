Building Function:
def build_SeparatedEASYmodel(input_shape1, input_shape2, input_shape3):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(128, kernel_size=40, strides=8, activation='relu', name='conv1d_1_1')(input1)
    x1 = MaxPooling1D(pool_size=2, name='maxpool1d_1_1')(x1)
    x1 = Conv1D(128, kernel_size=20, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = MaxPooling1D(pool_size=2, name='maxpool1d_1_2')(x1)
    x1 = Conv1D(256, kernel_size=4, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalMaxPooling1D(name='globalmaxpool1d_1_1')(x1)
    x1 = Dense(128, activation='relu', name='dense_1_1')(x1)
    x1 = Dropout(0.2, name='dropout_1_1')(x1)
    out1 = Dense(64, activation='relu', name='dense_1_2')(x1)

    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(128, kernel_size=2, activation='relu', name='conv1d_2_1')(input2)
    x2 = MaxPooling1D(pool_size=2, name='maxpool1d_2_1')(x2)
    x2 = Conv1D(64, kernel_size=8, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = MaxPooling1D(pool_size=2, name='maxpool1d_2_2')(x2)
    x2 = Conv1D(128, kernel_size=4, activation='relu', name='conv1d_2_3')(x2)
    x2 = Conv1D(64, kernel_size=2, activation='relu', name='conv1d_2_4')(x2)
    x2 = GlobalMaxPooling1D(name='globalmaxpool1d_2_1')(x2)
    x2 = Dropout(0.1, name='dropout_2_1')(x2)
    out2 = Dense(64, activation='relu', name='dense_2_1')(x2)

    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(64, kernel_size=40, strides=10, activation='relu', name='conv1d_3_1')(input3)
    x3 = MaxPooling1D(pool_size=2, name='maxpool1d_3_1')(x3)
    x3 = Conv1D(64, kernel_size=10, activation='relu', name='conv1d_3_2')(x3)
    x3 = MaxPooling1D(pool_size=2, name='maxpool1d_3_2')(x3)
    x3 = Flatten(name='flatten_3_1')(x3)
    out3 = Dense(32, activation='relu', name='dense_3_1')(x3)

    # Concatenate the outputs of the branches
    merged = concatenate([out1, out2, out3], name='concatenate_1')
    merged = Dropout(0.2, name='dropout_merged')(merged)
    merged = Dense(64, activation='relu', name='dense_merged_1')(merged)
    output = Dense(1, activation='sigmoid', name='output')(merged)

    # Create the model
    separated_model = Model(inputs=[input1, input2, input3], outputs=output)

    return separated_model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = np.dstack((data[:, :, 0], data[:, :, 2]))
        globals()[f"{key}2"] = np.dstack((data[:, :, 0], data[:, :, 2]))
        globals()[f"{key}3"] = np.dstack((data[:, :, 6], data[:, :, 8]))
        # Uncomment and modify the line below if you need the fourth set
        # globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_4"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 800, 2)    │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 800, 2)    │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 800, 2)    │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_0 (Conv1D) │ (None, 80, 128)   │     25,728 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_0 (Conv1D) │ (None, 80, 64)    │     12,864 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_0 (Conv1D) │ (None, 80, 64)    │     12,864 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 19, 128)   │    131,200 │ conv1d_1_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 19, 128)   │     65,664 │ conv1d_2_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 19, 128)   │     65,664 │ conv1d_3_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 6, 256)    │    262,400 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 6, 128)    │    131,200 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 6, 128)    │    131,200 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 6, 256)    │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 6, 128)    │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 6, 128)    │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 5, 512)    │    262,656 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 5, 128)    │     32,896 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 5, 128)    │     32,896 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 512)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 128)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 128)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 768)       │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 128)       │     98,432 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      2,064 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 1)         │         17 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 3,803,237 (14.51 MB)
 Trainable params: 1,267,745 (4.84 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 2,535,492 (9.67 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f5e0c26ec50>
Loss Function: binary_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.1620139330625534
Test val_loss: 0.37424665689468384
Train accuracy: 0.9362912178039551
Accuracy Score: 0.9249146757679181
F1 Score: 0.9329268292682927
Classification Report:
               precision    recall  f1-score   support

         0.0       0.92      0.91      0.91       130
         1.0       0.93      0.94      0.93       163

    accuracy                           0.92       293
   macro avg       0.92      0.92      0.92       293
weighted avg       0.92      0.92      0.92       293

Training History:
accuracy: [0.5426621437072754, 0.5517633557319641, 0.5665528774261475, 0.5972696542739868, 0.5699658989906311, 0.5449374318122864, 0.6222980618476868, 0.6291239857673645, 0.6268486976623535, 0.6188850998878479, 0.6143344640731812, 0.61774742603302, 0.6268486976623535, 0.6393629312515259, 0.6655290126800537, 0.6496018171310425, 0.6985210180282593, 0.6825938820838928, 0.7064846158027649, 0.7098976373672485, 0.6985210180282593, 0.6962457299232483, 0.6905574798583984, 0.7531285285949707, 0.7679181098937988, 0.7565415501594543, 0.7736063599586487, 0.8327645063400269, 0.8418657779693604, 0.8361774682998657, 0.8668941855430603, 0.8543799519538879, 0.8873720169067383, 0.8976109027862549, 0.8816837072372437, 0.9044368863105774, 0.9044368863105774, 0.9215016961097717, 0.9101251363754272, 0.9180887341499329, 0.8976109027862549, 0.8998862504959106, 0.916951060295105, 0.9192264080047607, 0.939704179763794, 0.914675772190094, 0.9249146580696106, 0.9453924894332886, 0.9453924894332886, 0.935153603553772, 0.9362912178039551]
loss: [0.7079750895500183, 0.6892024278640747, 0.6821837425231934, 0.6645161509513855, 0.6538901329040527, 0.6682592034339905, 0.6248576641082764, 0.6029836535453796, 0.6059609651565552, 0.6173816919326782, 0.5984023213386536, 0.6361913084983826, 0.6046531200408936, 0.6012454628944397, 0.5766990184783936, 0.5867637991905212, 0.5604907870292664, 0.5688225030899048, 0.5531717538833618, 0.5537880659103394, 0.5424996018409729, 0.5467069149017334, 0.5645557045936584, 0.49144476652145386, 0.47005170583724976, 0.4692426919937134, 0.4296651780605316, 0.3956126272678375, 0.364644855260849, 0.3820745348930359, 0.3439420759677887, 0.3104685842990875, 0.2496006190776825, 0.2433931827545166, 0.28618425130844116, 0.22871732711791992, 0.22478964924812317, 0.1913423091173172, 0.20987844467163086, 0.20231573283672333, 0.2513294219970703, 0.24054016172885895, 0.2072286605834961, 0.19365140795707703, 0.15859481692314148, 0.19757026433944702, 0.16514745354652405, 0.13414202630519867, 0.13828226923942566, 0.15446244180202484, 0.1620139330625534]
val_accuracy: [0.4846416413784027, 0.5563139915466309, 0.57337886095047, 0.5494880676269531, 0.552901029586792, 0.5870307087898254, 0.6040955781936646, 0.6484641432762146, 0.658703088760376, 0.6006826162338257, 0.5767918229103088, 0.6075085401535034, 0.6655290126800537, 0.6621160507202148, 0.6860068440437317, 0.7167235612869263, 0.7133105993270874, 0.6245733499526978, 0.7133105993270874, 0.6894198060035706, 0.7098976373672485, 0.7064846158027649, 0.7372013926506042, 0.8020477890968323, 0.7610921263694763, 0.7747440338134766, 0.744027316570282, 0.8191125988960266, 0.8634812235832214, 0.8225256204605103, 0.8566552996635437, 0.8532423377037048, 0.8668941855430603, 0.8771331310272217, 0.873720109462738, 0.8805460929870605, 0.873720109462738, 0.8430033922195435, 0.8805460929870605, 0.8634812235832214, 0.8668941855430603, 0.8771331310272217, 0.8600682616233826, 0.8771331310272217, 0.8873720169067383, 0.8668941855430603, 0.8703071475028992, 0.8532423377037048, 0.8771331310272217, 0.8873720169067383, 0.8532423377037048]
val_loss: [0.6925466656684875, 0.6805711984634399, 0.6632622480392456, 0.6736838817596436, 0.6524093747138977, 0.6378344893455505, 0.6254615187644958, 0.6149562001228333, 0.5910176038742065, 0.6187582612037659, 0.6702553033828735, 0.6303426623344421, 0.6350637674331665, 0.5841223001480103, 0.5836105942726135, 0.5905942916870117, 0.5822288990020752, 0.580668568611145, 0.5813158750534058, 0.5652751326560974, 0.5615373849868774, 0.5921873450279236, 0.5331601500511169, 0.42767736315727234, 0.46617600321769714, 0.4597528278827667, 0.4715202748775482, 0.40089449286460876, 0.40590617060661316, 0.43963029980659485, 0.34420841932296753, 0.3274913728237152, 0.358530730009079, 0.3124941885471344, 0.3317798376083374, 0.29509827494621277, 0.3334784507751465, 0.3885178565979004, 0.28496912121772766, 0.3953106999397278, 0.3432126045227051, 0.3038157820701599, 0.36605024337768555, 0.3392975628376007, 0.4029533267021179, 0.3758930265903473, 0.3318883776664734, 0.4607318043708801, 0.40644600987434387, 0.3222428262233734, 0.37424665689468384]

Confusion Matrix:
[[118  12]
 [ 10 153]]

################################################################################################ 

