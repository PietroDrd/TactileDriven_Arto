Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64, kernel_size=100, strides=20, activation='relu', padding='same', name='conv1d_1_0')(input1)
    x1 = Conv1D(filters=128, kernel_size=20, strides=8, activation='relu', name='conv1d_1_1')(x1)
    x1 = Conv1D(filters=256, kernel_size=8, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = Dropout(rate=0.2, name='dropout_1_1')(x1)
    x1 = Conv1D(filters=256, kernel_size=2, strides=1, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)

    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64, kernel_size=100, strides=20, activation='relu', padding='same', name='conv1d_2_0')(input2)
    x2 = Conv1D(filters=128, kernel_size=20, strides=8, activation='relu', name='conv1d_2_1')(x2)
    x2 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = Dropout(rate=0.2, name='dropout_2_1')(x2)
    x2 = Conv1D(filters=64, kernel_size=2, strides=1, activation='relu', name='conv1d_2_3')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)

    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64, kernel_size=100, strides=20, activation='relu', padding='same', name='conv1d_3_0')(input3)
    x3 = Conv1D(filters=128, kernel_size=20, strides=8, activation='relu', name='conv1d_3_1')(x3)
    x3 = Conv1D(filters=256, kernel_size=8, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = Dropout(rate=0.2, name='dropout_3_1')(x3)
    x3 = Conv1D(filters=256, kernel_size=2, strides=1, activation='relu', name='conv1d_3_3')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)

    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64, kernel_size=100, strides=20, activation='relu', padding='same', name='conv1d_4_0')(input4)
    x4 = Conv1D(filters=128, kernel_size=20, strides=8, activation='relu', name='conv1d_4_1')(x4)
    x4 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_4_2')(x4)
    x4 = Dropout(rate=0.2, name='dropout_4_1')(x4)
    x4 = Conv1D(filters=64, kernel_size=2, strides=1, activation='relu', name='conv1d_4_3')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)

    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')

    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)

    # Output layer for 6-class classification
    output = Dense(6, activation='softmax', name='output')(dense)

    # Create the model
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)

    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 1]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 4]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_31"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_0 (Conv1D) │ (None, 150, 64)   │      6,464 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_0 (Conv1D) │ (None, 150, 64)   │      6,464 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_0 (Conv1D) │ (None, 150, 64)   │     12,864 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_0 (Conv1D) │ (None, 150, 64)   │     12,864 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 17, 128)   │    163,968 │ conv1d_1_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 17, 128)   │    163,968 │ conv1d_2_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 17, 128)   │    163,968 │ conv1d_3_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 17, 128)   │    163,968 │ conv1d_4_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 5, 256)    │    262,400 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 5, 128)    │    131,200 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 5, 256)    │    262,400 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 5, 128)    │    131,200 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 5, 256)    │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 5, 128)    │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 5, 256)    │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_4_1         │ (None, 5, 128)    │          0 │ conv1d_4_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 4, 256)    │    131,328 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 4, 64)     │     16,448 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 4, 256)    │    131,328 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_3 (Conv1D) │ (None, 4, 64)     │     16,448 │ dropout_4_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 64)        │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 64)        │          0 │ conv1d_4_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 640)       │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0],  │
│                     │                   │            │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     41,024 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        390 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 5,456,084 (20.81 MB)
 Trainable params: 1,818,694 (6.94 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 3,637,390 (13.88 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adamw.AdamW object at 0x7f87b449fbb0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adamw/learning_rate>

Train loss: 0.08151654154062271
Test val_loss: 0.3208368718624115
Train accuracy: 0.9651162624359131
Accuracy Score: 0.9651162790697675
F1 Score: 0.9658914728682171
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      1.00      1.00        17
         1.0       1.00      1.00      1.00        25
         2.0       0.95      0.91      0.93        23
         3.0       0.91      0.95      0.93        21

    accuracy                           0.97        86
   macro avg       0.97      0.97      0.97        86
weighted avg       0.97      0.97      0.97        86

Training History:
accuracy: [0.5426356792449951, 0.7054263353347778, 0.7906976938247681, 0.8488371968269348, 0.7713178396224976, 0.8372092843055725, 0.8720930218696594, 0.8720930218696594, 0.856589138507843, 0.7596899271011353, 0.8100775480270386, 0.8682170510292053, 0.8798449635505676, 0.8837209343910217, 0.8914728760719299, 0.8914728760719299, 0.856589138507843, 0.9069767594337463, 0.895348846912384, 0.8643410801887512, 0.856589138507843, 0.8875969052314758, 0.8294573426246643, 0.8604651093482971, 0.9108527302742004, 0.9186046719551086, 0.895348846912384, 0.9418604373931885, 0.934108555316925, 0.9147287011146545, 0.930232584476471, 0.8682170510292053, 0.9108527302742004, 0.9379844665527344, 0.934108555316925, 0.9573643207550049, 0.9224806427955627, 0.9186046719551086, 0.9418604373931885, 0.9651162624359131, 0.9651162624359131, 0.9651162624359131, 0.9573643207550049, 0.9031007885932922, 0.8914728760719299, 0.9379844665527344, 0.961240291595459, 0.9651162624359131]
loss: [1.1786494255065918, 0.8940186500549316, 0.5135749578475952, 0.3803817927837372, 0.5054675340652466, 0.3680976629257202, 0.28093937039375305, 0.2717306315898895, 0.318543016910553, 0.5629913806915283, 0.4608386754989624, 0.34027308225631714, 0.2808002531528473, 0.27800682187080383, 0.2602604031562805, 0.3044118583202362, 0.2688034772872925, 0.2522268295288086, 0.2382722795009613, 0.3314790427684784, 0.2837194502353668, 0.23852941393852234, 0.40348589420318604, 0.262471467256546, 0.21304439008235931, 0.2003300040960312, 0.20998075604438782, 0.1688116490840912, 0.16290250420570374, 0.19258247315883636, 0.15951363742351532, 0.2487197071313858, 0.20114940404891968, 0.14808273315429688, 0.13302026689052582, 0.1350099742412567, 0.1857728511095047, 0.17403605580329895, 0.16080443561077118, 0.10697197914123535, 0.09239645302295685, 0.07795896381139755, 0.0914442166686058, 0.2838843762874603, 0.2359168529510498, 0.14909052848815918, 0.12342347204685211, 0.08151654154062271]
val_accuracy: [0.6162790656089783, 0.8139534592628479, 0.8488371968269348, 0.8255813717842102, 0.7441860437393188, 0.8023256063461304, 0.8372092843055725, 0.7674418687820435, 0.7093023061752319, 0.6860465407371521, 0.8372092843055725, 0.8372092843055725, 0.8372092843055725, 0.8488371968269348, 0.8255813717842102, 0.7441860437393188, 0.7906976938247681, 0.8255813717842102, 0.7906976938247681, 0.8023256063461304, 0.8023256063461304, 0.8255813717842102, 0.8604651093482971, 0.7906976938247681, 0.8255813717842102, 0.8372092843055725, 0.8488371968269348, 0.8488371968269348, 0.895348846912384, 0.8488371968269348, 0.8372092843055725, 0.895348846912384, 0.8604651093482971, 0.8139534592628479, 0.8720930218696594, 0.8255813717842102, 0.8720930218696594, 0.8372092843055725, 0.8837209343910217, 0.8837209343910217, 0.8837209343910217, 0.8837209343910217, 0.8720930218696594, 0.7674418687820435, 0.8604651093482971, 0.8604651093482971, 0.8604651093482971, 0.9186046719551086]
val_loss: [1.077445149421692, 0.5880565047264099, 0.41562891006469727, 0.42506372928619385, 0.5132263898849487, 0.396604984998703, 0.42083197832107544, 0.6185003519058228, 0.6214308142662048, 0.6267051100730896, 0.36118245124816895, 0.3434959053993225, 0.4502656161785126, 0.4328975975513458, 0.460095077753067, 0.4968668818473816, 0.42017099261283875, 0.3395869731903076, 0.3974880278110504, 0.39027461409568787, 0.44681787490844727, 0.3544432520866394, 0.3098508417606354, 0.5303744673728943, 0.41512441635131836, 0.43781787157058716, 0.34083321690559387, 0.3871003985404968, 0.3159315288066864, 0.37510260939598083, 0.3088526725769043, 0.3279905319213867, 0.36318567395210266, 0.41579127311706543, 0.3698047399520874, 0.384493350982666, 0.41576266288757324, 0.42049679160118103, 0.2774205207824707, 0.2763753831386566, 0.3291243314743042, 0.3594347834587097, 0.48540198802948, 0.5892189145088196, 0.36129945516586304, 0.2952496409416199, 0.32354724407196045, 0.3208368718624115]

################################################################################################ 

