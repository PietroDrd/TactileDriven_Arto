def build_TwoBranchModel(F_input_shape, S_input_shape, num_classes):
    # 1D Data Branch
    input_1D = tf.keras.layers.Input(shape=F_input_shape, name='input_1D')
    conv1_1D = tf.keras.layers.Conv1D(64, kernel_size=40, strides=10, name='conv1_1D')(input_1D)
    conv1_1D = tf.keras.layers.Activation('relu')(conv1_1D)
    pool1_1D = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1_1D)

    conv2_1D = tf.keras.layers.Conv1D(128, kernel_size=8, strides=4, name='conv2_1D')(pool1_1D)
    conv2_1D = tf.keras.layers.Activation('relu')(conv2_1D)
    #pool2_1D = tf.keras.layers.GlobalMaxPooling1D()(conv2_1D)

    conv3_1D = tf.keras.layers.Conv1D(128, kernel_size=2, strides=1, name='conv3_1D')(conv2_1D)
    conv3_1D = tf.keras.layers.Activation('relu')(conv3_1D)
    pool3_1D = tf.keras.layers.GlobalMaxPooling1D()(conv3_1D)

    flatten_1D = tf.keras.layers.Flatten()(pool3_1D)

    # 2D Data Branch (Scaleograms)
    input_2D = tf.keras.layers.Input(shape=S_input_shape, name='input_2D')
    conv1_2D = tf.keras.layers.Conv2D(64, kernel_size=(20, 20), strides=(10,10), padding='same', name='conv1_2D')(input_2D)
    conv1_2D = tf.keras.layers.Activation('relu')(conv1_2D)
    pool1_2D = tf.keras.layers.MaxPooling2D(pool_size=(4, 4))(conv1_2D)

    conv2_2D = tf.keras.layers.Conv2D(128, kernel_size=(10, 10), strides=(5,5), padding='same', name='conv2_2D')(pool1_2D)
    conv2_2D = tf.keras.layers.Activation('relu')(conv2_2D)
    pool2_2D = tf.keras.layers.GlobalMaxPooling2D()(conv2_2D)

    flatten_2D = tf.keras.layers.Flatten()(pool2_2D)

    # Merge branches
    merged = tf.keras.layers.concatenate([flatten_1D, flatten_2D])

    # Fully Connected Layers
    fc = tf.keras.layers.Dense(128, activation='relu')(merged)
    fc = tf.keras.layers.Dense(64, activation='relu')(fc)
    output = tf.keras.layers.Dense(num_classes, activation='softmax')(fc)

    # Define the Model
    model = tf.keras.Model(inputs=[input_1D, input_2D], outputs=output, name='TwoBranchModel')
    return model

Model: "TwoBranchModel"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input_1D            │ (None, 3000, 9)   │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1_1D (Conv1D)   │ (None, 297, 64)   │     23,104 │ input_1D[0][0]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_8        │ (None, 297, 64)   │          0 │ conv1_1D[0][0]    │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input_2D            │ (None, 3000, 768, │          0 │ -                 │
│ (InputLayer)        │ 1)                │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_2     │ (None, 148, 64)   │          0 │ activation_8[0][… │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1_2D (Conv2D)   │ (None, 300, 77,   │     25,664 │ input_2D[0][0]    │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2_1D (Conv1D)   │ (None, 36, 128)   │     65,664 │ max_pooling1d_2[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_11       │ (None, 300, 77,   │          0 │ conv1_2D[0][0]    │
│ (Activation)        │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_9        │ (None, 36, 128)   │          0 │ conv2_1D[0][0]    │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_2     │ (None, 75, 19,    │          0 │ activation_11[0]… │
│ (MaxPooling2D)      │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv3_1D (Conv1D)   │ (None, 35, 128)   │     32,896 │ activation_9[0][… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2_2D (Conv2D)   │ (None, 15, 4,     │    819,328 │ max_pooling2d_2[… │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_10       │ (None, 35, 128)   │          0 │ conv3_1D[0][0]    │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_12       │ (None, 15, 4,     │          0 │ conv2_2D[0][0]    │
│ (Activation)        │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ global_max_pooling… │ (None, 128)       │          0 │ activation_10[0]… │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ global_max_pooling… │ (None, 128)       │          0 │ activation_12[0]… │
│ (GlobalMaxPooling2… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_4 (Flatten) │ (None, 128)       │          0 │ global_max_pooli… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_5 (Flatten) │ (None, 128)       │          0 │ global_max_pooli… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_2       │ (None, 256)       │          0 │ flatten_4[0][0],  │
│ (Concatenate)       │                   │            │ flatten_5[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_6 (Dense)     │ (None, 128)       │     32,896 │ concatenate_2[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_7 (Dense)     │ (None, 64)        │      8,256 │ dense_6[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_8 (Dense)     │ (None, 4)         │        260 │ dense_7[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 3,024,206 (11.54 MB)
 Trainable params: 1,008,068 (3.85 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 2,016,138 (7.69 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f7b1c4ca440>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.08420108258724213
Test val_loss: 0.23407083749771118
Train accuracy: 0.9651162624359131
Accuracy Score: 0.9418604651162791
F1 Score: 0.9425354637979222
Classification Report:
               precision    recall  f1-score   support

         0.0       0.94      1.00      0.97        17
         1.0       1.00      0.96      0.98        25
         2.0       0.91      0.87      0.89        23
         3.0       0.91      0.95      0.93        21

    accuracy                           0.94        86
   macro avg       0.94      0.95      0.94        86
weighted avg       0.94      0.94      0.94        86

Training History:
accuracy: [0.40697672963142395, 0.5503876209259033, 0.7674418687820435, 0.6899224519729614, 0.7015503644943237, 0.7945736646652222, 0.8333333134651184, 0.8527131676673889, 0.9186046719551086, 0.8759689927101135, 0.930232584476471, 0.8720930218696594, 0.8992248177528381, 0.9379844665527344, 0.9457364082336426, 0.9457364082336426, 0.8410852551460266, 0.8139534592628479, 0.9069767594337463, 0.9379844665527344, 0.8875969052314758, 0.9108527302742004, 0.934108555316925, 0.9379844665527344, 0.9496123790740967, 0.9573643207550049, 0.8837209343910217, 0.9418604373931885, 0.9651162624359131, 0.9728682041168213, 0.9573643207550049, 0.9728682041168213, 0.8682170510292053, 0.856589138507843, 0.9186046719551086, 0.930232584476471, 0.9534883499145508, 0.9651162624359131, 0.9728682041168213, 0.9418604373931885, 0.9651162624359131, 0.9728682041168213, 0.9806201457977295, 0.9651162624359131]
loss: [1.4264382123947144, 1.085036039352417, 0.5661135911941528, 0.9229617714881897, 0.7798172235488892, 0.484201580286026, 0.35008642077445984, 0.3018285632133484, 0.24744826555252075, 0.2857013940811157, 0.19793042540550232, 0.3328924775123596, 0.24081897735595703, 0.16404661536216736, 0.12710201740264893, 0.13178841769695282, 0.45387133955955505, 0.5360482931137085, 0.2319495528936386, 0.18640053272247314, 0.27705731987953186, 0.17106257379055023, 0.17816360294818878, 0.19374792277812958, 0.14394992589950562, 0.14600805938243866, 0.3229324221611023, 0.13671565055847168, 0.09898870438337326, 0.09016330540180206, 0.119211345911026, 0.08915893733501434, 0.3733358383178711, 0.3178468346595764, 0.25188106298446655, 0.17071303725242615, 0.12962113320827484, 0.10777275264263153, 0.08576727658510208, 0.14527183771133423, 0.08449677377939224, 0.07279053330421448, 0.07197821140289307, 0.08420108258724213]
val_accuracy: [0.2906976640224457, 0.6162790656089783, 0.6162790656089783, 0.6627907156944275, 0.569767415523529, 0.8023256063461304, 0.7906976938247681, 0.8255813717842102, 0.7209302186965942, 0.8604651093482971, 0.8139534592628479, 0.7906976938247681, 0.8488371968269348, 0.8837209343910217, 0.8837209343910217, 0.7906976938247681, 0.7790697813034058, 0.8720930218696594, 0.8372092843055725, 0.9069767594337463, 0.8604651093482971, 0.8255813717842102, 0.895348846912384, 0.895348846912384, 0.895348846912384, 0.8604651093482971, 0.8488371968269348, 0.895348846912384, 0.8720930218696594, 0.9069767594337463, 0.9069767594337463, 0.8372092843055725, 0.8720930218696594, 0.7674418687820435, 0.8488371968269348, 0.9069767594337463, 0.895348846912384, 0.895348846912384, 0.8837209343910217, 0.9069767594337463, 0.895348846912384, 0.9069767594337463, 0.8837209343910217, 0.930232584476471]
val_loss: [1.2740025520324707, 0.8072852492332458, 1.2262383699417114, 0.7880994081497192, 0.8286212682723999, 0.42203786969184875, 0.4767782390117645, 0.4714096486568451, 0.6979419589042664, 0.34843483567237854, 0.4467830955982208, 0.5874110460281372, 0.38678503036499023, 0.31572040915489197, 0.4087350070476532, 0.5213912129402161, 0.4780235290527344, 0.4007798433303833, 0.4396740794181824, 0.2845074534416199, 0.3819037973880768, 0.501043975353241, 0.3138315975666046, 0.31152161955833435, 0.35769927501678467, 0.48336103558540344, 0.339836448431015, 0.27365803718566895, 0.3336634933948517, 0.2836163640022278, 0.3150499165058136, 0.4092963933944702, 0.3248234987258911, 0.5171809792518616, 0.3582124710083008, 0.21820390224456787, 0.21971526741981506, 0.2472444623708725, 0.3882123827934265, 0.23662079870700836, 0.3089037537574768, 0.2542727589607239, 0.28144246339797974, 0.23407083749771118]

################################################################################################ 

