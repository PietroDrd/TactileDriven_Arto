Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    x1 = Flatten()(x1)
        # it was GlobalAveragePooling1D
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    x2 = Flatten()(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    x3 = Flatten()(x3)
    
    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_4_1')(input4)
    x4 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_4_2')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)
    x4 = Flatten()(x4)
    
    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')
    
    # Dense layers
    dense = Dense(128, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 1]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 4]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_14"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_40          │ (None, 256)       │          0 │ gap1d_1_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_41          │ (None, 256)       │          0 │ gap1d_2_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_42          │ (None, 256)       │          0 │ gap1d_3_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_43          │ (None, 256)       │          0 │ gap1d_4_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ flatten_40[0][0], │
│ (Concatenate)       │                   │            │ flatten_41[0][0], │
│                     │                   │            │ flatten_42[0][0], │
│                     │                   │            │ flatten_43[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 128)       │    131,200 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 4)         │        516 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 2,064,782 (7.88 MB)
 Trainable params: 688,260 (2.63 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,376,522 (5.25 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f20b0760400>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.12439543008804321
Test val_loss: 0.3379282057285309
Train accuracy: 0.9689922332763672
Accuracy Score: 0.9651162790697675
F1 Score: 0.9655508013110332
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        17
         1.0       1.00      0.96      0.98        25
         2.0       0.92      0.96      0.94        23
         3.0       0.95      1.00      0.98        21

    accuracy                           0.97        86
   macro avg       0.97      0.96      0.97        86
weighted avg       0.97      0.97      0.97        86

Training History:
accuracy: [0.38759690523147583, 0.5542635917663574, 0.6705426573753357, 0.6705426573753357, 0.7248061895370483, 0.7596899271011353, 0.7674418687820435, 0.7790697813034058, 0.786821722984314, 0.7674418687820435, 0.8100775480270386, 0.7790697813034058, 0.8139534592628479, 0.751937985420227, 0.7596899271011353, 0.8410852551460266, 0.7829457521438599, 0.7093023061752319, 0.8410852551460266, 0.9108527302742004, 0.8643410801887512, 0.8992248177528381, 0.9108527302742004, 0.9263566136360168, 0.9224806427955627, 0.9379844665527344, 0.9069767594337463, 0.9224806427955627, 0.9147287011146545, 0.8488371968269348, 0.9224806427955627, 0.895348846912384, 0.9108527302742004, 0.8992248177528381, 0.930232584476471, 0.9108527302742004, 0.9147287011146545, 0.8759689927101135, 0.8643410801887512, 0.8449612259864807, 0.9496123790740967, 0.934108555316925, 0.9379844665527344, 0.9534883499145508, 0.9689922332763672]
loss: [1.441530466079712, 1.016390085220337, 0.870531439781189, 0.7381612658500671, 0.6360087394714355, 0.5740397572517395, 0.5270186066627502, 0.4422552287578583, 0.4781046509742737, 0.49042102694511414, 0.440409392118454, 0.5395187735557556, 0.44379910826683044, 0.501642107963562, 0.5260419249534607, 0.3810829520225525, 0.5698854327201843, 0.7450932264328003, 0.36570972204208374, 0.2731916904449463, 0.30635133385658264, 0.2651820778846741, 0.2252805083990097, 0.19048449397087097, 0.17552731931209564, 0.15941831469535828, 0.2250858098268509, 0.2067580670118332, 0.2256872057914734, 0.3158261775970459, 0.21246369183063507, 0.21578407287597656, 0.22579531371593475, 0.2885687053203583, 0.16017931699752808, 0.25248077511787415, 0.19268324971199036, 0.3020704984664917, 0.5982548594474792, 0.31499356031417847, 0.17310819029808044, 0.1575063318014145, 0.17076526582241058, 0.14357109367847443, 0.12439543008804321]
val_accuracy: [0.3604651093482971, 0.5930232405662537, 0.5, 0.6860465407371521, 0.6279069781303406, 0.8139534592628479, 0.8139534592628479, 0.8255813717842102, 0.6511628031730652, 0.6976743936538696, 0.7558139562606812, 0.7325581312179565, 0.8139534592628479, 0.7441860437393188, 0.7441860437393188, 0.6976743936538696, 0.6279069781303406, 0.7790697813034058, 0.8139534592628479, 0.8372092843055725, 0.8837209343910217, 0.8372092843055725, 0.8255813717842102, 0.9069767594337463, 0.9069767594337463, 0.8488371968269348, 0.930232584476471, 0.8023256063461304, 0.8720930218696594, 0.9186046719551086, 0.9069767594337463, 0.8372092843055725, 0.930232584476471, 0.895348846912384, 0.8604651093482971, 0.8139534592628479, 0.8139534592628479, 0.7674418687820435, 0.8604651093482971, 0.8372092843055725, 0.9418604373931885, 0.9069767594337463, 0.8837209343910217, 0.9069767594337463, 0.8720930218696594]
val_loss: [1.2139918804168701, 1.0885242223739624, 1.0686790943145752, 0.8587706685066223, 0.8004595041275024, 0.5695592164993286, 0.49715378880500793, 0.47294360399246216, 0.6171825528144836, 0.5722739100456238, 0.6136183738708496, 0.609528660774231, 0.4559393525123596, 0.493792325258255, 0.5578452348709106, 0.7767375707626343, 1.1119379997253418, 0.5013461709022522, 0.45096227526664734, 0.3909532427787781, 0.29804691672325134, 0.37249916791915894, 0.36137655377388, 0.2599616050720215, 0.24746966361999512, 0.34258875250816345, 0.2317107617855072, 0.4184314012527466, 0.3278595805168152, 0.21286676824092865, 0.23331193625926971, 0.4168737232685089, 0.2047906517982483, 0.2747454345226288, 0.3792973756790161, 0.44428136944770813, 0.42392757534980774, 0.745245099067688, 0.2592373788356781, 0.4511249363422394, 0.27151399850845337, 0.2539026141166687, 0.2822021543979645, 0.23712775111198425, 0.3379282057285309]

################################################################################################ 

