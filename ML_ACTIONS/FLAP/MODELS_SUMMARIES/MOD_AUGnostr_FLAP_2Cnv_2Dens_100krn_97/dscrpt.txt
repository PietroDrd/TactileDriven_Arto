Building Function:
def build_branched_model(input_shapes):
    def create_branch(input_shape, branch_id):
        input_layer = Input(shape=input_shape, name=f'input{branch_id}')
        x = Conv1D(filters=64*FILTN, kernel_size=100, strides=10, activation='relu', padding='same', name=f'conv1d_{branch_id}_1')(input_layer)
        x = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name=f'conv1d_{branch_id}_2')(x)
        # x = Dropout(rate=0.2, name=f'dropout_{branch_id}_1')(x)
        # x = Conv1D(filters=256*FILTN, kernel_size=2, strides=1, activation='relu', name=f'conv1d_{branch_id}_3')(x)
        x = GlobalMaxPooling1D(name=f'gap1d_{branch_id}_1')(x)
        return input_layer, x

    inputs = []
    branches = []
    
    for i, input_shape in enumerate(input_shapes, 1):
        input_layer, branch_output = create_branch(input_shape, i)
        inputs.append(input_layer)
        branches.append(branch_output)
    
    merged = concatenate(branches, name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=inputs, outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = np.dstack((data[:, :, 0], data[:, :, 4]))
        globals()[f"{key}2"] = np.dstack((data[:, :, 1], data[:, :, 3]))
        globals()[f"{key}3"] = np.dstack((data[:, :, 2],))
        # Uncomment and modify the line below if you need the fourth set
        # globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_1"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 1800, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 1800, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 1800, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 180, 128)  │     25,728 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 180, 128)  │     25,728 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 180, 128)  │     12,928 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 89, 256)   │    131,328 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 89, 256)   │    131,328 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 89, 256)   │    131,328 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 768)       │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     49,216 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      1,040 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 3)         │         51 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,526,027 (5.82 MB)
 Trainable params: 508,675 (1.94 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,017,352 (3.88 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f4d6c68c8e0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.06351049244403839
Test val_loss: 0.25955089926719666
Train accuracy: 0.9764957427978516
Accuracy Score: 0.9745222929936306
F1 Score: 0.9746828051175878
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.96      0.98        47
         1.0       0.98      0.97      0.97        59
         2.0       0.94      1.00      0.97        51

    accuracy                           0.97       157
   macro avg       0.98      0.97      0.97       157
weighted avg       0.98      0.97      0.97       157

Training History:
accuracy: [0.44017094373703003, 0.6089743375778198, 0.6004273295402527, 0.7115384340286255, 0.7564102411270142, 0.752136766910553, 0.8226495981216431, 0.8611111044883728, 0.867521345615387, 0.882478654384613, 0.882478654384613, 0.8696581125259399, 0.8653846383094788, 0.9017093777656555, 0.9145299196243286, 0.9059829115867615, 0.938034176826477, 0.8803418874740601, 0.9102563858032227, 0.938034176826477, 0.9273504018783569, 0.9444444179534912, 0.9529914259910583, 0.9465811848640442, 0.9123931527137756, 0.9059829115867615, 0.9123931527137756, 0.9465811848640442, 0.9508547186851501, 0.9700854420661926, 0.9529914259910583, 0.942307710647583, 0.9209401607513428, 0.9487179517745972, 0.942307710647583, 0.9465811848640442, 0.9273504018783569, 0.9679487347602844, 0.9615384340286255, 0.9764957427978516, 0.9871794581413269, 0.9807692170143127, 0.9807692170143127, 0.9829059839248657, 0.9700854420661926, 0.9722222089767456, 0.9871794581413269, 0.9914529919624329, 0.9786324501037598, 0.9786324501037598, 0.9871794581413269, 0.9914529919624329, 0.9764957427978516, 0.9786324501037598, 0.9764957427978516]
loss: [1.0564656257629395, 0.8646815419197083, 0.8094058036804199, 0.6697924733161926, 0.6026991009712219, 0.6017391085624695, 0.45399209856987, 0.3803650736808777, 0.36440324783325195, 0.328201025724411, 0.3096482753753662, 0.3042660057544708, 0.35582053661346436, 0.255425363779068, 0.23664167523384094, 0.25397372245788574, 0.18641473352909088, 0.2697705030441284, 0.23119334876537323, 0.17823752760887146, 0.2122020125389099, 0.15099577605724335, 0.13504545390605927, 0.1492895931005478, 0.2282831370830536, 0.2585461735725403, 0.21933670341968536, 0.14966551959514618, 0.10755796730518341, 0.10519210249185562, 0.13415491580963135, 0.1406465619802475, 0.166402667760849, 0.11367721110582352, 0.1331191062927246, 0.12550389766693115, 0.1489086151123047, 0.08791470527648926, 0.08673088252544403, 0.06073248013854027, 0.04771695286035538, 0.04391974210739136, 0.04731036722660065, 0.0527651272714138, 0.08689974248409271, 0.07581782341003418, 0.05069335550069809, 0.037070583552122116, 0.048573918640613556, 0.04830506816506386, 0.04020316153764725, 0.034753184765577316, 0.060614682734012604, 0.0610685721039772, 0.06351049244403839]
val_accuracy: [0.557692289352417, 0.44871795177459717, 0.7243589758872986, 0.6346153616905212, 0.7884615659713745, 0.8205128312110901, 0.8205128312110901, 0.8525640964508057, 0.7307692170143127, 0.8653846383094788, 0.8461538553237915, 0.8910256624221802, 0.8012820482254028, 0.8846153616905212, 0.8846153616905212, 0.8846153616905212, 0.8782051205635071, 0.8205128312110901, 0.8782051205635071, 0.9230769276618958, 0.8846153616905212, 0.9102563858032227, 0.8717948794364929, 0.9102563858032227, 0.807692289352417, 0.8717948794364929, 0.8717948794364929, 0.9102563858032227, 0.9102563858032227, 0.8782051205635071, 0.9038461446762085, 0.8782051205635071, 0.8141025900840759, 0.8974359035491943, 0.9038461446762085, 0.9230769276618958, 0.9358974099159241, 0.8846153616905212, 0.9166666865348816, 0.9358974099159241, 0.9294871687889099, 0.942307710647583, 0.9358974099159241, 0.9358974099159241, 0.9038461446762085, 0.942307710647583, 0.942307710647583, 0.9038461446762085, 0.9358974099159241, 0.9294871687889099, 0.9358974099159241, 0.9166666865348816, 0.8910256624221802, 0.9230769276618958, 0.9358974099159241]
val_loss: [0.8763208389282227, 0.9617536664009094, 0.704937756061554, 0.7602183222770691, 0.5849746465682983, 0.5298405885696411, 0.4701194167137146, 0.4155462682247162, 0.6065468192100525, 0.38107725977897644, 0.4036555290222168, 0.3535787761211395, 0.4648635685443878, 0.32538115978240967, 0.3099927008152008, 0.32049623131752014, 0.30907419323921204, 0.4208690822124481, 0.3383309245109558, 0.2548869550228119, 0.3344046175479889, 0.25506120920181274, 0.27699577808380127, 0.24416916072368622, 0.609149694442749, 0.3316545784473419, 0.27195581793785095, 0.24495701491832733, 0.24233807623386383, 0.26800212264060974, 0.2409386783838272, 0.3844252824783325, 0.41858163475990295, 0.2231953889131546, 0.2918984591960907, 0.20959235727787018, 0.19401438534259796, 0.2432650476694107, 0.22159790992736816, 0.18790584802627563, 0.20054179430007935, 0.2102150022983551, 0.1863432228565216, 0.208857461810112, 0.24028229713439941, 0.21615314483642578, 0.17050419747829437, 0.20991051197052002, 0.2074727565050125, 0.20107144117355347, 0.1993335336446762, 0.24854691326618195, 0.2924148440361023, 0.3025839924812317, 0.25955089926719666]

Confusion Matrix:
[[45  1  1]
 [ 0 57  2]
 [ 0  0 51]]

################################################################################################ 

