Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    x1 = Flatten()(x1)
        # it was GlobalAveragePooling1D
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    x2 = Flatten()(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    x3 = Flatten()(x3)
    
    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_4_1')(input4)
    x4 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_4_2')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)
    x4 = Flatten()(x4)
    
    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 2]
        globals()[f"{key}2"] = data[:, :, 1]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 3]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_1"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_4 (Flatten) │ (None, 256)       │          0 │ gap1d_1_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_5 (Flatten) │ (None, 256)       │          0 │ gap1d_2_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_6 (Flatten) │ (None, 256)       │          0 │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_7 (Flatten) │ (None, 256)       │          0 │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ flatten_4[0][0],  │
│ (Concatenate)       │                   │            │ flatten_5[0][0],  │
│                     │                   │            │ flatten_6[0][0],  │
│                     │                   │            │ flatten_7[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     65,600 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 4)         │        260 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,867,214 (7.12 MB)
 Trainable params: 622,404 (2.37 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,244,810 (4.75 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f094dbf34f0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.1891741007566452
Test val_loss: 0.19026818871498108
Train accuracy: 0.9418604373931885
Accuracy Score: 0.9651162790697675
F1 Score: 0.9670918367346939
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      1.00      1.00        17
         1.0       1.00      0.96      0.98        25
         2.0       0.88      1.00      0.94        23
         3.0       1.00      0.90      0.95        21

    accuracy                           0.97        86
   macro avg       0.97      0.97      0.97        86
weighted avg       0.97      0.97      0.97        86

Training History:
accuracy: [0.3255814015865326, 0.46124032139778137, 0.6085271239280701, 0.5968992114067078, 0.6666666865348816, 0.7325581312179565, 0.7441860437393188, 0.6937984228134155, 0.751937985420227, 0.7829457521438599, 0.7170542478561401, 0.7945736646652222, 0.682170569896698, 0.7906976938247681, 0.817829430103302, 0.817829430103302, 0.8255813717842102, 0.856589138507843, 0.8255813717842102, 0.7635658979415894, 0.8372092843055725, 0.8294573426246643, 0.8488371968269348, 0.8759689927101135, 0.8255813717842102, 0.8682170510292053, 0.9108527302742004, 0.8837209343910217, 0.8720930218696594, 0.8992248177528381, 0.9031007885932922, 0.9186046719551086, 0.8837209343910217, 0.7790697813034058, 0.8410852551460266, 0.8643410801887512, 0.8875969052314758, 0.9186046719551086, 0.895348846912384, 0.934108555316925, 0.9457364082336426, 0.9379844665527344, 0.7674418687820435, 0.8255813717842102, 0.8798449635505676, 0.8372092843055725, 0.934108555316925, 0.9379844665527344, 0.9457364082336426, 0.9263566136360168, 0.930232584476471, 0.9418604373931885, 0.8449612259864807, 0.9224806427955627, 0.9186046719551086, 0.9147287011146545, 0.9224806427955627, 0.9108527302742004, 0.9379844665527344, 0.9573643207550049, 0.930232584476471, 0.9457364082336426, 0.9418604373931885, 0.9573643207550049, 0.9457364082336426, 0.9496123790740967, 0.8720930218696594, 0.8837209343910217, 0.9379844665527344, 0.9418604373931885]
loss: [1.4352468252182007, 1.1218384504318237, 0.9297477602958679, 0.8839671015739441, 0.7113537788391113, 0.5965384244918823, 0.5340753197669983, 0.6412696838378906, 0.5312010049819946, 0.49075964093208313, 0.4896979331970215, 0.4672924280166626, 0.6211494207382202, 0.46282586455345154, 0.3958863914012909, 0.4090937077999115, 0.4055648148059845, 0.38827967643737793, 0.39207202196121216, 0.4670635759830475, 0.37461063265800476, 0.4153009057044983, 0.34253448247909546, 0.31390225887298584, 0.3675360381603241, 0.3246014714241028, 0.2941322922706604, 0.2921386659145355, 0.2761273980140686, 0.23766137659549713, 0.23943115770816803, 0.22883184254169464, 0.2688591182231903, 0.47841888666152954, 0.3414778411388397, 0.28585100173950195, 0.26348114013671875, 0.22941356897354126, 0.2253890037536621, 0.17612290382385254, 0.17005561292171478, 0.1647917926311493, 0.7005037665367126, 0.42680487036705017, 0.2875097692012787, 0.30188244581222534, 0.19657965004444122, 0.18076248466968536, 0.1658712774515152, 0.16822363436222076, 0.15771329402923584, 0.16002026200294495, 0.3412324786186218, 0.23623515665531158, 0.2227594256401062, 0.20192766189575195, 0.22161594033241272, 0.19359464943408966, 0.13519705832004547, 0.13060486316680908, 0.17556948959827423, 0.1745336800813675, 0.14201976358890533, 0.1256270855665207, 0.12519696354866028, 0.13406284153461456, 0.27938178181648254, 0.2556355893611908, 0.16057188808918, 0.1891741007566452]
val_accuracy: [0.38372093439102173, 0.44186046719551086, 0.604651153087616, 0.6279069781303406, 0.6279069781303406, 0.6627907156944275, 0.7790697813034058, 0.6860465407371521, 0.7325581312179565, 0.7674418687820435, 0.7790697813034058, 0.6395348906517029, 0.6744186282157898, 0.7790697813034058, 0.7790697813034058, 0.7325581312179565, 0.8139534592628479, 0.8139534592628479, 0.7093023061752319, 0.7325581312179565, 0.7906976938247681, 0.7325581312179565, 0.8139534592628479, 0.8488371968269348, 0.8139534592628479, 0.8372092843055725, 0.8604651093482971, 0.8604651093482971, 0.895348846912384, 0.8255813717842102, 0.8488371968269348, 0.8720930218696594, 0.8837209343910217, 0.6860465407371521, 0.9069767594337463, 0.8023256063461304, 0.9069767594337463, 0.9186046719551086, 0.9186046719551086, 0.9069767594337463, 0.9069767594337463, 0.7441860437393188, 0.930232584476471, 0.8139534592628479, 0.9069767594337463, 0.8604651093482971, 0.895348846912384, 0.930232584476471, 0.895348846912384, 0.930232584476471, 0.8720930218696594, 0.8837209343910217, 0.8837209343910217, 0.895348846912384, 0.9534883499145508, 0.8488371968269348, 0.9186046719551086, 0.9186046719551086, 0.930232584476471, 0.9534883499145508, 0.8720930218696594, 0.9534883499145508, 0.9534883499145508, 0.9186046719551086, 0.930232584476471, 0.9069767594337463, 0.8139534592628479, 0.8488371968269348, 0.8720930218696594, 0.9069767594337463]
val_loss: [1.253343939781189, 1.1204077005386353, 0.9277862906455994, 0.7943994998931885, 0.7339833974838257, 0.6280519962310791, 0.622093677520752, 0.6057029366493225, 0.5009998083114624, 0.5021220445632935, 0.5052128434181213, 0.7264684438705444, 0.7211599349975586, 0.48068034648895264, 0.4345259666442871, 0.4695248305797577, 0.49111565947532654, 0.41505396366119385, 0.5088338851928711, 0.5627692341804504, 0.4990980327129364, 0.5578431487083435, 0.3993467390537262, 0.343603253364563, 0.4820133149623871, 0.34119343757629395, 0.39886555075645447, 0.38688206672668457, 0.2658296525478363, 0.3576343059539795, 0.35229653120040894, 0.30293694138526917, 0.26024025678634644, 0.8238685131072998, 0.2562611401081085, 0.44266366958618164, 0.3007082939147949, 0.2604646384716034, 0.20624423027038574, 0.2244279831647873, 0.22310838103294373, 0.6161988973617554, 0.2032719999551773, 0.4989562928676605, 0.23655754327774048, 0.29767894744873047, 0.22848039865493774, 0.20971760153770447, 0.23899435997009277, 0.182329460978508, 0.32165324687957764, 0.2924808859825134, 0.38648366928100586, 0.23436519503593445, 0.15401796996593475, 0.29075887799263, 0.16868793964385986, 0.21871279180049896, 0.16694217920303345, 0.1744978278875351, 0.30106791853904724, 0.143411785364151, 0.16845615208148956, 0.1973097175359726, 0.1496754288673401, 0.17844675481319427, 0.5453442931175232, 0.229898601770401, 0.2793591022491455, 0.19026818871498108]

Confusion Matrix:
[[17  0  0  0]
 [ 0 24  1  0]
 [ 0  0 23  0]
 [ 0  0  2 19]]

################################################################################################ 

