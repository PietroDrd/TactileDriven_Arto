Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64, kernel_size=10, strides=4, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = Conv1D(filters=128, kernel_size=4, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    x1 = Flatten()(x1)
        # it was GlobalAveragePooling1D
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64, kernel_size=10, strides=4, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = Conv1D(filters=128, kernel_size=4, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    x2 = Flatten()(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64, kernel_size=10, strides=4, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = Conv1D(filters=128, kernel_size=4, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    x3 = Flatten()(x3)
    
    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64, kernel_size=10, strides=4, activation='relu', padding='same', name='conv1d_4_1')(input4)
    x4 = Conv1D(filters=128, kernel_size=4, strides=2, activation='relu', name='conv1d_4_2')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)
    x4 = Flatten()(x4)
    
    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 1]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 4]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_5"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 750, 64)   │        704 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 750, 64)   │        704 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 750, 64)   │      1,344 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 750, 64)   │      1,344 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 374, 128)  │     32,896 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 374, 128)  │     32,896 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 374, 128)  │     32,896 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 374, 128)  │     32,896 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 128)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 128)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 128)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 128)       │          0 │ conv1d_4_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_4 (Flatten) │ (None, 128)       │          0 │ gap1d_1_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_5 (Flatten) │ (None, 128)       │          0 │ gap1d_2_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_6 (Flatten) │ (None, 128)       │          0 │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_7 (Flatten) │ (None, 128)       │          0 │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 512)       │          0 │ flatten_4[0][0],  │
│ (Concatenate)       │                   │            │ flatten_5[0][0],  │
│                     │                   │            │ flatten_6[0][0],  │
│                     │                   │            │ flatten_7[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     32,832 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      1,040 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 4)         │         68 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 508,862 (1.94 MB)
 Trainable params: 169,620 (662.58 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 339,242 (1.29 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f20f02afd90>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.0937635749578476
Test val_loss: 0.2815243899822235
Train accuracy: 0.9689922332763672
Accuracy Score: 0.9534883720930233
F1 Score: 0.9541889483065953
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        17
         1.0       0.96      1.00      0.98        25
         2.0       0.88      0.96      0.92        23
         3.0       1.00      0.90      0.95        21

    accuracy                           0.95        86
   macro avg       0.96      0.95      0.95        86
weighted avg       0.96      0.95      0.95        86

Training History:
accuracy: [0.3100775182247162, 0.38372093439102173, 0.4806201457977295, 0.5968992114067078, 0.6356589198112488, 0.6589147448539734, 0.6666666865348816, 0.7054263353347778, 0.6356589198112488, 0.6705426573753357, 0.7635658979415894, 0.786821722984314, 0.748062014579773, 0.7984496355056763, 0.8023256063461304, 0.7713178396224976, 0.7596899271011353, 0.8139534592628479, 0.7984496355056763, 0.8488371968269348, 0.8875969052314758, 0.8333333134651184, 0.7984496355056763, 0.8139534592628479, 0.8604651093482971, 0.8759689927101135, 0.8604651093482971, 0.8759689927101135, 0.8914728760719299, 0.8798449635505676, 0.895348846912384, 0.895348846912384, 0.9147287011146545, 0.9031007885932922, 0.934108555316925, 0.9263566136360168, 0.9379844665527344, 0.9418604373931885, 0.9379844665527344, 0.9224806427955627, 0.8992248177528381, 0.9418604373931885, 0.9379844665527344, 0.9418604373931885, 0.9457364082336426, 0.9496123790740967, 0.9379844665527344, 0.9263566136360168, 0.9534883499145508, 0.9418604373931885, 0.9457364082336426, 0.961240291595459, 0.9689922332763672, 0.9689922332763672, 0.961240291595459, 0.9728682041168213, 0.9573643207550049, 0.9689922332763672, 0.9651162624359131, 0.9689922332763672, 0.9573643207550049, 0.9573643207550049, 0.9689922332763672, 0.9573643207550049, 0.9806201457977295, 0.9689922332763672, 0.930232584476471, 0.9689922332763672]
loss: [1.3336291313171387, 1.202757716178894, 1.098169207572937, 0.949426531791687, 0.8985394239425659, 0.8000470995903015, 0.7332596182823181, 0.6543281078338623, 0.8789084553718567, 0.7082720398902893, 0.5880420804023743, 0.5681226253509521, 0.5515086650848389, 0.4898757338523865, 0.44657474756240845, 0.49853646755218506, 0.4904100298881531, 0.4366961717605591, 0.48632943630218506, 0.4176414906978607, 0.37072890996932983, 0.4714028239250183, 0.47140952944755554, 0.4806671738624573, 0.3807675838470459, 0.3229072093963623, 0.35088831186294556, 0.2930193841457367, 0.2850019931793213, 0.3510354161262512, 0.298965185880661, 0.26309946179389954, 0.26025936007499695, 0.27418115735054016, 0.22178110480308533, 0.2043754905462265, 0.18526777625083923, 0.180461585521698, 0.1795186549425125, 0.19791069626808167, 0.23313289880752563, 0.1783633679151535, 0.17768365144729614, 0.17858271300792694, 0.14670529961585999, 0.15715853869915009, 0.19435955584049225, 0.16820253431797028, 0.1396131068468094, 0.16964420676231384, 0.15657441318035126, 0.13154001533985138, 0.12042979151010513, 0.1267307996749878, 0.12704020738601685, 0.11102106422185898, 0.11484048515558243, 0.10800828784704208, 0.11640140414237976, 0.1111341044306755, 0.10602894425392151, 0.11896524578332901, 0.10749286413192749, 0.12211328744888306, 0.08996402472257614, 0.10091966390609741, 0.16574953496456146, 0.0937635749578476]
val_accuracy: [0.3720930218696594, 0.27906978130340576, 0.5930232405662537, 0.5, 0.6162790656089783, 0.6744186282157898, 0.6627907156944275, 0.5232558250427246, 0.6744186282157898, 0.7093023061752319, 0.6860465407371521, 0.6744186282157898, 0.6976743936538696, 0.7674418687820435, 0.7558139562606812, 0.6976743936538696, 0.6860465407371521, 0.8023256063461304, 0.6860465407371521, 0.8139534592628479, 0.7906976938247681, 0.7790697813034058, 0.6744186282157898, 0.8139534592628479, 0.7906976938247681, 0.7674418687820435, 0.8488371968269348, 0.8372092843055725, 0.8488371968269348, 0.6976743936538696, 0.8604651093482971, 0.8604651093482971, 0.7790697813034058, 0.8488371968269348, 0.895348846912384, 0.9069767594337463, 0.9069767594337463, 0.9069767594337463, 0.9069767594337463, 0.8720930218696594, 0.8837209343910217, 0.895348846912384, 0.8604651093482971, 0.895348846912384, 0.895348846912384, 0.8604651093482971, 0.895348846912384, 0.8837209343910217, 0.9186046719551086, 0.8720930218696594, 0.9069767594337463, 0.9069767594337463, 0.9069767594337463, 0.9186046719551086, 0.9069767594337463, 0.895348846912384, 0.9069767594337463, 0.9186046719551086, 0.9186046719551086, 0.9186046719551086, 0.9186046719551086, 0.9186046719551086, 0.895348846912384, 0.930232584476471, 0.9186046719551086, 0.895348846912384, 0.9186046719551086, 0.9069767594337463]
val_loss: [1.2799345254898071, 1.218238353729248, 1.0556952953338623, 1.0241366624832153, 0.9078524112701416, 0.7619836330413818, 0.7438193559646606, 0.9996749758720398, 0.7105294466018677, 0.6073867678642273, 0.7268848419189453, 0.6645476222038269, 0.5615968704223633, 0.5504381656646729, 0.6427680850028992, 0.5990886092185974, 0.5931627750396729, 0.4537973701953888, 0.557974636554718, 0.45782235264778137, 0.6052985191345215, 0.5727761387825012, 0.8382844924926758, 0.411685049533844, 0.43607109785079956, 0.49277567863464355, 0.3702227473258972, 0.38175448775291443, 0.34232404828071594, 0.4618968963623047, 0.3434426188468933, 0.3170814514160156, 0.4828495681285858, 0.2860396206378937, 0.30504342913627625, 0.2545691430568695, 0.2533695697784424, 0.26073896884918213, 0.2542153298854828, 0.2698548436164856, 0.23326395452022552, 0.27098703384399414, 0.28577569127082825, 0.23076216876506805, 0.2594761550426483, 0.2756361961364746, 0.2988477349281311, 0.2830166518688202, 0.2726391851902008, 0.3297998607158661, 0.23113982379436493, 0.20433856546878815, 0.2177051454782486, 0.23525209724903107, 0.23959508538246155, 0.22079136967658997, 0.2115510255098343, 0.22902479767799377, 0.21412254869937897, 0.1953154057264328, 0.23120570182800293, 0.20598968863487244, 0.23152007162570953, 0.21399633586406708, 0.20917870104312897, 0.25264787673950195, 0.21284939348697662, 0.2815243899822235]

################################################################################################ 

