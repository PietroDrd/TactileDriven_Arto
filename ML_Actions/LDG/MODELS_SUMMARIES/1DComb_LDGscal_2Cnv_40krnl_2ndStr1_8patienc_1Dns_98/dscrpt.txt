Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = Conv1D(filters=128*FILTN, kernel_size=4, strides=1, activation='relu', name='conv1d_1_2')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    x1 = Flatten()(x1)
        # it was GlobalAveragePooling1D
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = Conv1D(filters=128*FILTN, kernel_size=4, strides=1, activation='relu', name='conv1d_2_2')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    x2 = Flatten()(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = Conv1D(filters=128*FILTN, kernel_size=4, strides=1, activation='relu', name='conv1d_3_2')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    x3 = Flatten()(x3)
    
    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_4_1')(input4)
    x4 = Conv1D(filters=128*FILTN, kernel_size=4, strides=1, activation='relu', name='conv1d_4_2')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)
    x4 = Flatten()(x4)
    
    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')
    
    # Dense layers
    dense = Dense(128, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 1]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 4]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_12"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 297, 256)  │    131,328 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 297, 256)  │    131,328 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 297, 256)  │    131,328 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 297, 256)  │    131,328 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_48          │ (None, 256)       │          0 │ gap1d_1_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_49          │ (None, 256)       │          0 │ gap1d_2_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_50          │ (None, 256)       │          0 │ gap1d_3_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_51          │ (None, 256)       │          0 │ gap1d_4_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ flatten_48[0][0], │
│ (Concatenate)       │                   │            │ flatten_49[0][0], │
│                     │                   │            │ flatten_50[0][0], │
│                     │                   │            │ flatten_51[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 128)       │    131,200 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 4)         │        516 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 2,064,782 (7.88 MB)
 Trainable params: 688,260 (2.63 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,376,522 (5.25 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adamw.AdamW object at 0x7f88d023f520>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adamw/learning_rate>

Train loss: 0.14159968495368958
Test val_loss: 0.2345471978187561
Train accuracy: 0.9573643207550049
Accuracy Score: 0.9767441860465116
F1 Score: 0.9769055349412492
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        17
         1.0       1.00      0.96      0.98        25
         2.0       0.92      1.00      0.96        23
         3.0       1.00      1.00      1.00        21

    accuracy                           0.98        86
   macro avg       0.98      0.98      0.98        86
weighted avg       0.98      0.98      0.98        86

Training History:
accuracy: [0.43410852551460266, 0.45736435055732727, 0.6666666865348816, 0.6627907156944275, 0.7054263353347778, 0.7364341020584106, 0.7596899271011353, 0.7558139562606812, 0.7906976938247681, 0.8449612259864807, 0.8449612259864807, 0.817829430103302, 0.8875969052314758, 0.8294573426246643, 0.6744186282157898, 0.7558139562606812, 0.8333333134651184, 0.817829430103302, 0.8217054009437561, 0.8720930218696594, 0.8837209343910217, 0.8759689927101135, 0.9224806427955627, 0.8875969052314758, 0.8643410801887512, 0.9224806427955627, 0.934108555316925, 0.9224806427955627, 0.930232584476471, 0.9379844665527344, 0.9534883499145508, 0.9418604373931885, 0.9108527302742004, 0.8992248177528381, 0.8837209343910217, 0.9573643207550049, 0.9689922332763672, 0.9651162624359131, 0.9689922332763672, 0.9418604373931885, 0.9496123790740967, 0.9457364082336426, 0.9457364082336426, 0.9573643207550049, 0.9651162624359131, 0.9651162624359131, 0.9651162624359131, 0.9806201457977295, 0.9689922332763672, 0.9224806427955627, 0.9573643207550049]
loss: [1.3541722297668457, 1.1516810655593872, 0.8183932900428772, 0.8258475065231323, 0.742616593837738, 0.5586044192314148, 0.5074260830879211, 0.5167860388755798, 0.47338077425956726, 0.3674066662788391, 0.36543911695480347, 0.3882610499858856, 0.3346226215362549, 0.37943196296691895, 0.9485945701599121, 0.5953470468521118, 0.4066019654273987, 0.3948255479335785, 0.36850059032440186, 0.3772810995578766, 0.3309771716594696, 0.3498145043849945, 0.2220720648765564, 0.2889796793460846, 0.27818700671195984, 0.19997304677963257, 0.19412188231945038, 0.1884046047925949, 0.21384331583976746, 0.17558591067790985, 0.15271371603012085, 0.15697769820690155, 0.2208404690027237, 0.20804619789123535, 0.24009524285793304, 0.1371656060218811, 0.12258386611938477, 0.1086442619562149, 0.11133682727813721, 0.13016095757484436, 0.12861783802509308, 0.12898918986320496, 0.13023944199085236, 0.11769555509090424, 0.08747182041406631, 0.09707152098417282, 0.08868131786584854, 0.07736912369728088, 0.08690781891345978, 0.20067644119262695, 0.14159968495368958]
val_accuracy: [0.43023255467414856, 0.5, 0.5813953280448914, 0.6395348906517029, 0.6395348906517029, 0.7906976938247681, 0.8255813717842102, 0.6511628031730652, 0.8488371968269348, 0.7790697813034058, 0.7441860437393188, 0.8837209343910217, 0.7906976938247681, 0.7441860437393188, 0.6627907156944275, 0.7441860437393188, 0.7790697813034058, 0.8023256063461304, 0.8372092843055725, 0.8255813717842102, 0.8139534592628479, 0.8604651093482971, 0.8255813717842102, 0.8604651093482971, 0.8372092843055725, 0.8604651093482971, 0.9069767594337463, 0.9069767594337463, 0.9069767594337463, 0.8837209343910217, 0.895348846912384, 0.8837209343910217, 0.8604651093482971, 0.8488371968269348, 0.9186046719551086, 0.9186046719551086, 0.9418604373931885, 0.895348846912384, 0.930232584476471, 0.8604651093482971, 0.9069767594337463, 0.8488371968269348, 0.9418604373931885, 0.9069767594337463, 0.9186046719551086, 0.9069767594337463, 0.9069767594337463, 0.930232584476471, 0.930232584476471, 0.8372092843055725, 0.930232584476471]
val_loss: [1.2700624465942383, 1.0904213190078735, 1.1119754314422607, 0.8109803199768066, 0.8702207207679749, 0.5511267185211182, 0.5186466574668884, 0.7283824682235718, 0.4292958378791809, 0.43716341257095337, 0.5403133630752563, 0.3632996678352356, 0.4601546823978424, 0.4761243462562561, 0.9546492099761963, 0.557092547416687, 0.5590271949768066, 0.4969787895679474, 0.3710048794746399, 0.3465028703212738, 0.3868620991706848, 0.30960336327552795, 0.3781740963459015, 0.47492462396621704, 0.39001739025115967, 0.3261954188346863, 0.36571004986763, 0.35401496291160583, 0.2515541911125183, 0.2738068699836731, 0.2843177914619446, 0.34369370341300964, 0.44475582242012024, 0.410974383354187, 0.2777367830276489, 0.2704281806945801, 0.23793701827526093, 0.2780821919441223, 0.25699174404144287, 0.3924316465854645, 0.28985223174095154, 0.4254135191440582, 0.2329498529434204, 0.2762196660041809, 0.26205137372016907, 0.29487016797065735, 0.24112950265407562, 0.2624667286872864, 0.2669481337070465, 0.4735028147697449, 0.2345471978187561]

################################################################################################ 

