Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    x1 = Flatten()(x1)
        # it was GlobalAveragePooling1D
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    x2 = Flatten()(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    x3 = Flatten()(x3)
    
    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_4_1')(input4)
    x4 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_4_2')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)
    x4 = Flatten()(x4)
    
    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 1]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 4]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_27"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_44          │ (None, 256)       │          0 │ gap1d_1_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_45          │ (None, 256)       │          0 │ gap1d_2_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_46          │ (None, 256)       │          0 │ gap1d_3_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_47          │ (None, 256)       │          0 │ gap1d_4_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ flatten_44[0][0], │
│ (Concatenate)       │                   │            │ flatten_45[0][0], │
│                     │                   │            │ flatten_46[0][0], │
│                     │                   │            │ flatten_47[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     65,600 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 4)         │        260 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,867,214 (7.12 MB)
 Trainable params: 622,404 (2.37 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,244,810 (4.75 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f8200934fa0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.05730920284986496
Test val_loss: 0.11811909079551697
Train accuracy: 0.9768637418746948
Accuracy Score: 0.9615384615384616
F1 Score: 0.9666429460950008
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      1.00      1.00        27
         1.0       1.00      1.00      1.00        28
         2.0       0.92      0.95      0.94        38
         3.0       0.94      0.92      0.93        37

    accuracy                           0.96       130
   macro avg       0.97      0.97      0.97       130
weighted avg       0.96      0.96      0.96       130

Training History:
accuracy: [0.4730077087879181, 0.647814929485321, 0.7249357104301453, 0.8123393058776855, 0.8277634978294373, 0.8483290672302246, 0.791773796081543, 0.8277634978294373, 0.8740360140800476, 0.9151670932769775, 0.9048843383789062, 0.897172212600708, 0.9203084707260132, 0.8791773915290833, 0.9151670932769775, 0.9357326626777649, 0.922879159450531, 0.9408740401268005, 0.9537274837493896, 0.9614396095275879, 0.9331619739532471, 0.9614396095275879, 0.9511567950248718, 0.9665809869766235, 0.9305912852287292, 0.9434447288513184, 0.9614396095275879, 0.9511567950248718, 0.9691516757011414, 0.9768637418746948, 0.9820051193237305, 0.974293053150177, 0.9511567950248718, 0.948586106300354, 0.9768637418746948, 0.9820051193237305, 0.9768637418746948, 0.974293053150177, 0.9768637418746948]
loss: [1.1067994832992554, 0.7916046380996704, 0.6036052703857422, 0.4665127694606781, 0.43429747223854065, 0.3697299063205719, 0.4302320182323456, 0.413287490606308, 0.31718724966049194, 0.2582699954509735, 0.25113049149513245, 0.24478377401828766, 0.20554858446121216, 0.35268738865852356, 0.21518339216709137, 0.16252264380455017, 0.185295969247818, 0.16396890580654144, 0.13593289256095886, 0.1128491684794426, 0.1477932184934616, 0.10349737852811813, 0.11732403188943863, 0.0903802290558815, 0.2100183069705963, 0.14711768925189972, 0.11560015380382538, 0.1150948777794838, 0.096258245408535, 0.07162444293498993, 0.07216693460941315, 0.06064137816429138, 0.11292602866888046, 0.10238975286483765, 0.06694041192531586, 0.05007217824459076, 0.05126529932022095, 0.0653267577290535, 0.05730920284986496]
val_accuracy: [0.6692307591438293, 0.7615384459495544, 0.7769230604171753, 0.8230769038200378, 0.8461538553237915, 0.7076923251152039, 0.8615384697914124, 0.9076923131942749, 0.8384615182876587, 0.892307698726654, 0.807692289352417, 0.807692289352417, 0.8230769038200378, 0.8999999761581421, 0.9153845906257629, 0.8692307472229004, 0.9692307710647583, 0.9615384340286255, 0.9615384340286255, 0.9461538195610046, 0.9307692050933838, 0.9692307710647583, 0.9615384340286255, 0.8999999761581421, 0.8999999761581421, 0.9615384340286255, 0.9692307710647583, 0.9230769276618958, 0.9692307710647583, 0.9384615421295166, 0.9769230484962463, 0.9230769276618958, 0.9153845906257629, 0.9615384340286255, 0.9384615421295166, 0.9307692050933838, 0.892307698726654, 0.9307692050933838, 0.9692307710647583]
val_loss: [0.7627333402633667, 0.5803400278091431, 0.5063046216964722, 0.45115768909454346, 0.41625484824180603, 0.5367670655250549, 0.34858226776123047, 0.29240381717681885, 0.33068281412124634, 0.26770180463790894, 0.404146283864975, 0.36966291069984436, 0.4198286831378937, 0.27707141637802124, 0.1799418330192566, 0.2859082520008087, 0.1599062830209732, 0.17170082032680511, 0.16565604507923126, 0.16259394586086273, 0.1819426566362381, 0.18013547360897064, 0.13039961457252502, 0.308572381734848, 0.22915083169937134, 0.15131080150604248, 0.1273924857378006, 0.17266719043254852, 0.11410856992006302, 0.16821633279323578, 0.10046510398387909, 0.22200652956962585, 0.1614736169576645, 0.11470626294612885, 0.149640291929245, 0.14764805138111115, 0.22357389330863953, 0.15444515645503998, 0.11811909079551697]

################################################################################################ 

