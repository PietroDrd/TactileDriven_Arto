Building Function:
def build_branched_model(input_shapes):
    def create_branch(input_shape, branch_id):
        input_layer = Input(shape=input_shape, name=f'input{branch_id}')
        x = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name=f'conv1d_{branch_id}_1')(input_layer)
        x = Conv1D(filters=128*FILTN, kernel_size=8, strides=2, activation='relu', name=f'conv1d_{branch_id}_2')(x)
        x = Dropout(rate=0.22, name=f'dropout_{branch_id}_1')(x)
        x = Conv1D(filters=256*FILTN, kernel_size=2, strides=1, activation='relu', name=f'conv1d_{branch_id}_3')(x)
        x = Dropout(rate=0.10, name=f'dropout_{branch_id}_2')(x)
        x = GlobalMaxPooling1D(name=f'gap1d_{branch_id}_1')(x)
        return input_layer, x

    inputs = []
    branches = []
    
    for i, input_shape in enumerate(input_shapes, 1):
        input_layer, branch_output = create_branch(input_shape, i)
        inputs.append(input_layer)
        branches.append(branch_output)
    
    merged = concatenate(branches, name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=inputs, outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = np.dstack((data[:, :, 0], data[:, :, 4]))
        globals()[f"{key}2"] = np.dstack((data[:, :, 1], data[:, :, 3]))
        globals()[f"{key}3"] = np.dstack((data[:, :, 2],))
        # Uncomment and modify the line below if you need the fourth set
        # globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_8"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 1800, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 1800, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 1800, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 180, 64)   │      5,184 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 180, 64)   │      5,184 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 180, 64)   │      2,624 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 87, 128)   │     65,664 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 87, 128)   │     65,664 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 87, 128)   │     65,664 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 87, 128)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 87, 128)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 87, 128)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 86, 256)   │     65,792 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 86, 256)   │     65,792 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 86, 256)   │     65,792 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_2         │ (None, 86, 256)   │          0 │ conv1d_1_3[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_2         │ (None, 86, 256)   │          0 │ conv1d_2_3[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_2         │ (None, 86, 256)   │          0 │ conv1d_3_3[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ dropout_1_2[0][0] │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ dropout_2_2[0][0] │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ dropout_3_2[0][0] │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 768)       │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     49,216 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      1,040 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 3)         │         51 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,373,003 (5.24 MB)
 Trainable params: 457,667 (1.75 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 915,336 (3.49 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f4c785d3fa0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.11015667766332626
Test val_loss: 0.25021034479141235
Train accuracy: 0.9508547186851501
Accuracy Score: 0.9681528662420382
F1 Score: 0.9691435401890315
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.98      0.99        47
         1.0       1.00      0.93      0.96        59
         2.0       0.91      1.00      0.95        51

    accuracy                           0.97       157
   macro avg       0.97      0.97      0.97       157
weighted avg       0.97      0.97      0.97       157

Training History:
accuracy: [0.45299145579338074, 0.5341880321502686, 0.561965823173523, 0.6025640964508057, 0.6025640964508057, 0.6474359035491943, 0.6987179517745972, 0.7735042572021484, 0.7585470080375671, 0.8247863054275513, 0.8461538553237915, 0.8397436141967773, 0.8269230723381042, 0.8696581125259399, 0.9017093777656555, 0.8952991366386414, 0.8995726704597473, 0.9209401607513428, 0.8782051205635071, 0.9017093777656555, 0.9188033938407898, 0.9252136945724487, 0.9188033938407898, 0.942307710647583, 0.9529914259910583, 0.942307710647583, 0.8995726704597473, 0.9059829115867615, 0.9081196784973145, 0.9508547186851501, 0.9444444179534912, 0.9636752009391785, 0.9615384340286255, 0.9508547186851501, 0.9508547186851501, 0.9529914259910583, 0.9594017267227173, 0.9636752009391785, 0.9572649598121643, 0.9508547186851501]
loss: [1.0902557373046875, 0.9371045827865601, 0.9224276542663574, 0.8135455250740051, 0.8301287293434143, 0.7873976826667786, 0.7417639493942261, 0.588720977306366, 0.594740629196167, 0.46837201714515686, 0.40702196955680847, 0.3892596960067749, 0.4443546533584595, 0.33268409967422485, 0.27643021941185, 0.26130619645118713, 0.2628404498100281, 0.21767862141132355, 0.2847064137458801, 0.26552602648735046, 0.21225740015506744, 0.18511205911636353, 0.17605052888393402, 0.15596269071102142, 0.1333562433719635, 0.14350810647010803, 0.25778064131736755, 0.18829086422920227, 0.19879941642284393, 0.12343518435955048, 0.1417091190814972, 0.11857792735099792, 0.10868166387081146, 0.11495315283536911, 0.1277943104505539, 0.14530009031295776, 0.1199393942952156, 0.09065461158752441, 0.10784761607646942, 0.11015667766332626]
val_accuracy: [0.6282051205635071, 0.4423076808452606, 0.6730769276618958, 0.6474359035491943, 0.692307710647583, 0.7564102411270142, 0.6089743375778198, 0.7435897588729858, 0.7435897588729858, 0.8397436141967773, 0.8846153616905212, 0.6858974099159241, 0.8653846383094788, 0.7884615659713745, 0.8461538553237915, 0.8141025900840759, 0.8846153616905212, 0.8461538553237915, 0.8333333134651184, 0.9102563858032227, 0.8717948794364929, 0.8910256624221802, 0.8974359035491943, 0.8205128312110901, 0.8782051205635071, 0.8846153616905212, 0.9038461446762085, 0.8205128312110901, 0.9102563858032227, 0.8846153616905212, 0.9294871687889099, 0.9294871687889099, 0.9102563858032227, 0.9038461446762085, 0.9102563858032227, 0.8974359035491943, 0.9038461446762085, 0.9230769276618958, 0.8461538553237915, 0.9166666865348816]
val_loss: [0.9684514999389648, 0.9821929931640625, 0.8811393976211548, 0.8267178535461426, 0.7670719623565674, 0.7367368936538696, 0.7999215126037598, 0.5999975800514221, 0.5513020157814026, 0.4756893813610077, 0.3952168822288513, 0.5851755738258362, 0.3856702744960785, 0.42671194672584534, 0.4381691813468933, 0.38213327527046204, 0.3027111887931824, 0.4098164737224579, 0.3658747375011444, 0.2851201295852661, 0.2988993227481842, 0.2744167447090149, 0.2576582431793213, 0.4190841615200043, 0.3130030333995819, 0.3233109414577484, 0.2490893453359604, 0.34699010848999023, 0.23297134041786194, 0.26243844628334045, 0.21795886754989624, 0.21047861874103546, 0.2388642579317093, 0.24431312084197998, 0.2141324281692505, 0.24964377284049988, 0.25843459367752075, 0.21364405751228333, 0.33463039994239807, 0.25021034479141235]

Confusion Matrix:
[[46  0  1]
 [ 0 55  4]
 [ 0  0 51]]

################################################################################################ 

