Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = MaxPooling1D(pool_size=2)(x1)
    x1 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    #x1 = Flatten()(x1)
        # it was GlobalAveragePooling1D
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = MaxPooling1D(pool_size=2)(x2)
    x2 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    #x2 = Flatten()(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = MaxPooling1D(pool_size=2)(x3)
    x3 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    #x3 = Flatten()(x3)
    
    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_4_1')(input4)
    x4 = MaxPooling1D(pool_size=2)(x4)
    x4 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_4_2')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)
    #x4 = Flatten()(x4)
    
    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')
    
    # Dense layers
    dense = Dense(128, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 2]
        globals()[f"{key}3"] = np.dstack((data[:, :, 0], data[:, :, 6]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 2], data[:, :, 8]))


Model: "functional_5"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 1800, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 1800, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 1800, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 1800, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 180, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 180, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 180, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 180, 128)  │     10,368 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_20    │ (None, 90, 128)   │          0 │ conv1d_1_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_21    │ (None, 90, 128)   │          0 │ conv1d_2_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_22    │ (None, 90, 128)   │          0 │ conv1d_3_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_23    │ (None, 90, 128)   │          0 │ conv1d_4_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 44, 256)   │    131,328 │ max_pooling1d_20… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 44, 256)   │    131,328 │ max_pooling1d_21… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 44, 256)   │    131,328 │ max_pooling1d_22… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 44, 256)   │    131,328 │ max_pooling1d_23… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0],  │
│                     │                   │            │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 128)       │    131,200 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 3)         │        387 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 2,064,395 (7.88 MB)
 Trainable params: 688,131 (2.63 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,376,264 (5.25 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7fce1c44dde0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.2727280259132385
Test val_loss: 0.6546487808227539
Train accuracy: 0.8819875717163086
Accuracy Score: 0.9259259259259259
F1 Score: 0.9273937725477494
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.97      0.99        34
         1.0       0.92      0.90      0.91        40
         2.0       0.86      0.91      0.89        34

    accuracy                           0.93       108
   macro avg       0.93      0.93      0.93       108
weighted avg       0.93      0.93      0.93       108

Training History:
accuracy: [0.40993788838386536, 0.6180124282836914, 0.6987577676773071, 0.7795031070709229, 0.8136646151542664, 0.8788819909095764, 0.8726708292961121, 0.8726708292961121, 0.8478260636329651, 0.8260869383811951, 0.888198733329773, 0.909937858581543, 0.9068322777748108, 0.8819875717163086, 0.888198733329773, 0.8944099545478821, 0.760869562625885, 0.782608687877655, 0.8260869383811951, 0.909937858581543, 0.8944099545478821, 0.9316770434379578, 0.9316770434379578, 0.9068322777748108, 0.9534161686897278, 0.9378882050514221, 0.9068322777748108, 0.9658384919166565, 0.9347826242446899, 0.9534161686897278, 0.9409937858581543, 0.9068322777748108, 0.8819875717163086]
loss: [1.1468638181686401, 0.8068174719810486, 0.6706538796424866, 0.5517222881317139, 0.4839102327823639, 0.35424813628196716, 0.3318230211734772, 0.30164581537246704, 0.447666198015213, 0.4261079430580139, 0.2871127128601074, 0.26362380385398865, 0.28611627221107483, 0.28569546341896057, 0.25357869267463684, 0.25408869981765747, 0.5299268364906311, 0.5307301878929138, 0.4125570058822632, 0.258171021938324, 0.2231309562921524, 0.1886749416589737, 0.16679875552654266, 0.20318672060966492, 0.14458264410495758, 0.14735378324985504, 0.3327016234397888, 0.138248473405838, 0.1635294258594513, 0.15457500517368317, 0.15736615657806396, 0.19332900643348694, 0.2727280259132385]
val_accuracy: [0.5700934529304504, 0.5607476830482483, 0.84112149477005, 0.7663551568984985, 0.7663551568984985, 0.7570093274116516, 0.7663551568984985, 0.7009345889091492, 0.8037382960319519, 0.7663551568984985, 0.8317757248878479, 0.7850467562675476, 0.7476635575294495, 0.7663551568984985, 0.84112149477005, 0.8504672646522522, 0.8785046935081482, 0.8037382960319519, 0.8130841255187988, 0.8691588640213013, 0.9065420627593994, 0.9158878326416016, 0.8878504633903503, 0.9158878326416016, 0.9345794320106506, 0.9065420627593994, 0.8037382960319519, 0.8971962332725525, 0.8785046935081482, 0.8130841255187988, 0.8130841255187988, 0.7009345889091492, 0.8130841255187988]
val_loss: [0.8939467668533325, 0.9972167015075684, 0.638233482837677, 0.6248645782470703, 0.5880757570266724, 0.5512125492095947, 0.6087032556533813, 0.8391593098640442, 0.542241632938385, 0.6871380805969238, 0.488365113735199, 0.5577903389930725, 0.8430131673812866, 0.6360625624656677, 0.474668025970459, 0.48403042554855347, 0.41497886180877686, 0.5562896132469177, 0.5326917171478271, 0.40815407037734985, 0.35325899720191956, 0.35789695382118225, 0.38777562975883484, 0.3534047603607178, 0.32566872239112854, 0.3688794672489166, 0.6362919211387634, 0.36019012331962585, 0.43348878622055054, 0.6264046430587769, 0.6615121364593506, 1.0478506088256836, 0.6546487808227539]

Confusion Matrix:
[[33  0  1]
 [ 0 36  4]
 [ 0  3 31]]

################################################################################################ 

