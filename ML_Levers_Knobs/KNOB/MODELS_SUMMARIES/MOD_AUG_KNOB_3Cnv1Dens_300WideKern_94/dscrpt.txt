
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = np.dstack((data[:,:,2], data[:,:,4]))
        globals()[f"{key}2"] = np.dstack((data[:,:,0], data[:,:,2]))
        globals()[f"{key}3"] = data[:,:,5]
        globals()[f"{key}4"] = np.dstack((data[:,:,1], data[:,:,3]))

def build_branched_model(input_shapes):
    def create_branch(input_shape, branch_id):
        input_layer = Input(shape=input_shape, name=f'input{branch_id}')
        x = Conv1D(filters=128, kernel_size=300, strides=40, activation='relu', padding='same', name=f'conv1d_{branch_id}_1')(input_layer)
        #x = MaxPooling1D(pool_size=2)(x)
        x = Conv1D(filters=256, kernel_size=16, strides=2, activation='relu', name=f'conv1d_{branch_id}_2')(x)
        #x = Conv1D(filters=256, kernel_size=8, strides=2, activation='relu', name=f'conv1d_{branch_id}_3')(x)
        #x = MaxPooling1D(pool_size=4)(x)
        x = Dropout(rate=0.25, name=f'dropout_{branch_id}_1')(x)
        x = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name=f'conv1d_{branch_id}_4')(x)
        x = GlobalMaxPooling1D(name=f'gap1d_{branch_id}_1')(x)
        return input_layer, x

    inputs = []
    branches = []
    
    for i, input_shape in enumerate(input_shapes, 1):
        input_layer, branch_output = create_branch(input_shape, i)
        inputs.append(input_layer)
        branches.append(branch_output)
    
    merged = concatenate(branches, name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=inputs, outputs=output)
    return model

Model: "functional_10"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 50, 128)   │     76,928 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 50, 128)   │     76,928 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 50, 128)   │     38,528 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 50, 128)   │     76,928 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 18, 256)   │    524,544 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 18, 256)   │    524,544 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 18, 256)   │    524,544 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 18, 256)   │    524,544 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 18, 256)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 18, 256)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 18, 256)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_4_1         │ (None, 18, 256)   │          0 │ conv1d_4_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_4 (Conv1D) │ (None, 17, 512)   │    262,656 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_4 (Conv1D) │ (None, 17, 512)   │    262,656 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_4 (Conv1D) │ (None, 17, 512)   │    262,656 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_4 (Conv1D) │ (None, 17, 512)   │    262,656 │ dropout_4_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 512)       │          0 │ conv1d_1_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 512)       │          0 │ conv1d_2_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 512)       │          0 │ conv1d_3_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 512)       │          0 │ conv1d_4_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 2048)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0],  │
│                     │                   │            │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │    131,136 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        390 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 10,648,916 (40.62 MB)
 Trainable params: 3,549,638 (13.54 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 7,099,278 (27.08 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7efe845e2350>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.1025216281414032
Test val_loss: 0.44902825355529785
Train accuracy: 0.9643493890762329
Accuracy Score: 0.9358288770053476
F1 Score: 0.9314337805115671
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.95      0.97        40
         1.0       0.96      0.91      0.93        54
         2.0       0.93      0.98      0.95        42
         3.0       0.93      1.00      0.97        14
         4.0       1.00      0.87      0.93        15
         5.0       0.77      0.91      0.83        22

    accuracy                           0.94       187
   macro avg       0.93      0.93      0.93       187
weighted avg       0.94      0.94      0.94       187

Training History:
accuracy: [0.3172905445098877, 0.5579322576522827, 0.638146162033081, 0.7112299203872681, 0.7932263612747192, 0.7950088977813721, 0.8181818127632141, 0.8360071182250977, 0.8253119587898254, 0.8859180212020874, 0.8983957171440125, 0.9376114010810852, 0.9126559495925903, 0.9358288645744324, 0.9180035591125488, 0.9233511686325073, 0.9358288645744324, 0.9286987781524658, 0.9465240836143494, 0.9304812550544739, 0.9358288645744324, 0.9376114010810852, 0.9376114010810852, 0.9500890970230103, 0.9483066201210022, 0.9215686321258545, 0.9411764740943909, 0.9411764740943909, 0.9126559495925903, 0.9376114010810852, 0.9376114010810852, 0.9447415471076965, 0.9590017795562744, 0.9732620120048523, 0.9572192430496216, 0.9607843160629272, 0.9643493890762329]
loss: [1.7030326128005981, 1.1289604902267456, 0.9047670960426331, 0.7526918649673462, 0.5423164963722229, 0.4982849359512329, 0.42500993609428406, 0.42832449078559875, 0.47704532742500305, 0.31938791275024414, 0.2899693250656128, 0.20610474050045013, 0.24334444105625153, 0.1945164054632187, 0.2502209544181824, 0.2113749086856842, 0.2003336101770401, 0.20784519612789154, 0.1551014631986618, 0.20589810609817505, 0.17389997839927673, 0.20435583591461182, 0.18095581233501434, 0.13066473603248596, 0.12632521986961365, 0.22806383669376373, 0.17077283561229706, 0.20333842933177948, 0.25205695629119873, 0.15342898666858673, 0.1824933886528015, 0.14601869881153107, 0.1386624276638031, 0.09094084054231644, 0.12851715087890625, 0.10582947731018066, 0.1025216281414032]
val_accuracy: [0.45989304780960083, 0.5133689641952515, 0.6577540040016174, 0.6791443824768066, 0.8074866533279419, 0.7914438247680664, 0.7486631274223328, 0.7967914342880249, 0.8502673506736755, 0.8128342032432556, 0.866310179233551, 0.8342245817184448, 0.8181818127632141, 0.855614960193634, 0.8716577291488647, 0.8716577291488647, 0.8877005577087402, 0.8716577291488647, 0.8770053386688232, 0.8983957171440125, 0.8770053386688232, 0.893048107624054, 0.9304812550544739, 0.903743326663971, 0.9144384860992432, 0.866310179233551, 0.8877005577087402, 0.893048107624054, 0.8235294222831726, 0.8983957171440125, 0.903743326663971, 0.8823529481887817, 0.8128342032432556, 0.9144384860992432, 0.8395721912384033, 0.903743326663971, 0.903743326663971]
val_loss: [1.4111745357513428, 1.0967844724655151, 0.8368351459503174, 0.7941562533378601, 0.5067163705825806, 0.5681769251823425, 0.7443084716796875, 0.599916934967041, 0.5043548941612244, 0.47147080302238464, 0.35261812806129456, 0.4357973337173462, 0.5916707515716553, 0.473596453666687, 0.38392117619514465, 0.37022027373313904, 0.3447537422180176, 0.45700371265411377, 0.31270086765289307, 0.3780420422554016, 0.307539701461792, 0.40627530217170715, 0.3341354429721832, 0.32385697960853577, 0.2377992868423462, 0.40684959292411804, 0.48594197630882263, 0.5016269087791443, 0.5738168954849243, 0.37459927797317505, 0.318246454000473, 0.39997798204421997, 0.6506637930870056, 0.44787517189979553, 0.6763216853141785, 0.39054128527641296, 0.44902825355529785]

################################################################################################ 

