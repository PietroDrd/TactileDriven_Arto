def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = np.dstack((data[:,:,2], data[:,:,4]))
        globals()[f"{key}2"] = np.dstack((data[:,:,0], data[:,:,2]))
        globals()[f"{key}3"] = data[:,:,5]
        globals()[f"{key}4"] = np.dstack((data[:,:,1], data[:,:,3]))

def build_branched_model(input_shapes):
    def create_branch(input_shape, branch_id):
        input_layer = Input(shape=input_shape, name=f'input{branch_id}')
        x = Conv1D(filters=128, kernel_size=400, strides=40, activation='relu', padding='same', name=f'conv1d_{branch_id}_1')(input_layer)
        #x = MaxPooling1D(pool_size=2)(x)
        x = Conv1D(filters=256, kernel_size=16, strides=2, activation='relu', name=f'conv1d_{branch_id}_2')(x)
        x = Conv1D(filters=256, kernel_size=8, strides=2, activation='relu', name=f'conv1d_{branch_id}_3')(x)
        #x = MaxPooling1D(pool_size=4)(x)
        x = Dropout(rate=0.25, name=f'dropout_{branch_id}_1')(x)
        x = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name=f'conv1d_{branch_id}_4')(x)
        x = GlobalMaxPooling1D(name=f'gap1d_{branch_id}_1')(x)
        return input_layer, x

    inputs = []
    branches = []
    
    for i, input_shape in enumerate(input_shapes, 1):
        input_layer, branch_output = create_branch(input_shape, i)
        inputs.append(input_layer)
        branches.append(branch_output)
    
    merged = concatenate(branches, name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    #dense = Dense(32, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=inputs, outputs=output)
    return model

Model: "functional_7"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 50, 128)   │    102,528 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 50, 128)   │    102,528 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 50, 128)   │     51,328 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 50, 128)   │    102,528 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 18, 256)   │    524,544 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 18, 256)   │    524,544 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 18, 256)   │    524,544 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 18, 256)   │    524,544 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 6, 256)    │    524,544 │ conv1d_1_2[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 6, 256)    │    524,544 │ conv1d_2_2[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 6, 256)    │    524,544 │ conv1d_3_2[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_3 (Conv1D) │ (None, 6, 256)    │    524,544 │ conv1d_4_2[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 6, 256)    │          0 │ conv1d_1_3[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 6, 256)    │          0 │ conv1d_2_3[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 6, 256)    │          0 │ conv1d_3_3[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_4_1         │ (None, 6, 256)    │          0 │ conv1d_4_3[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_4 (Conv1D) │ (None, 5, 512)    │    262,656 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_4 (Conv1D) │ (None, 5, 512)    │    262,656 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_4 (Conv1D) │ (None, 5, 512)    │    262,656 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_4 (Conv1D) │ (None, 5, 512)    │    262,656 │ dropout_4_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 512)       │          0 │ conv1d_1_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 512)       │          0 │ conv1d_2_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 512)       │          0 │ conv1d_3_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 512)       │          0 │ conv1d_4_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 2048)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0],  │
│                     │                   │            │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │    131,136 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        390 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 17,212,244 (65.66 MB)
 Trainable params: 5,737,414 (21.89 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 11,474,830 (43.77 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7efedc1851b0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.1289101243019104
Test val_loss: 0.5538546442985535
Train accuracy: 0.9500890970230103
Accuracy Score: 0.9197860962566845
F1 Score: 0.8972111869091575
Classification Report:
               precision    recall  f1-score   support

         0.0       0.97      0.97      0.97        40
         1.0       0.94      0.93      0.93        54
         2.0       0.89      1.00      0.94        42
         3.0       0.82      1.00      0.90        14
         4.0       1.00      0.73      0.85        15
         5.0       0.84      0.73      0.78        22

    accuracy                           0.92       187
   macro avg       0.91      0.89      0.90       187
weighted avg       0.92      0.92      0.92       187

Training History:
accuracy: [0.3689839541912079, 0.4688057005405426, 0.614973247051239, 0.6720142364501953, 0.6934046149253845, 0.6880570650100708, 0.7557932138442993, 0.8163992762565613, 0.8163992762565613, 0.8253119587898254, 0.8894830942153931, 0.8841354846954346, 0.8983957171440125, 0.8983957171440125, 0.8199643492698669, 0.8770053386688232, 0.9126559495925903, 0.9251337051391602, 0.9340463280677795, 0.9090909361839294, 0.926916241645813, 0.9126559495925903, 0.9518716335296631, 0.9322637915611267, 0.9465240836143494, 0.9518716335296631, 0.9411764740943909, 0.9483066201210022, 0.9554367065429688, 0.9465240836143494, 0.9625668525695801, 0.9679144620895386, 0.9572192430496216, 0.9714794754981995, 0.9714794754981995, 0.9625668525695801, 0.9732620120048523, 0.9786096215248108, 0.9500890970230103]
loss: [1.5291186571121216, 1.160487174987793, 0.9144384860992432, 0.8330339789390564, 0.7472683787345886, 0.8068976402282715, 0.5870586037635803, 0.465779185295105, 0.46087440848350525, 0.49373987317085266, 0.32726094126701355, 0.31194716691970825, 0.29571300745010376, 0.2804510295391083, 0.568947434425354, 0.33760929107666016, 0.22422127425670624, 0.1670135259628296, 0.1849079728126526, 0.19858695566654205, 0.20071382820606232, 0.184953510761261, 0.14443905651569366, 0.1563929319381714, 0.1778072714805603, 0.13441960513591766, 0.16592983901500702, 0.13930945098400116, 0.10023260116577148, 0.12109210342168808, 0.08560580015182495, 0.07162526994943619, 0.10211476683616638, 0.07938005775213242, 0.06590916216373444, 0.08045294135808945, 0.07312880456447601, 0.07393421232700348, 0.1289101243019104]
val_accuracy: [0.46524062752723694, 0.5828877091407776, 0.6524063944816589, 0.5454545617103577, 0.7058823704719543, 0.6631016135215759, 0.7112299203872681, 0.8128342032432556, 0.7914438247680664, 0.7647058963775635, 0.8288770318031311, 0.8021390438079834, 0.8181818127632141, 0.8074866533279419, 0.8449198007583618, 0.759358286857605, 0.8449198007583618, 0.8609625697135925, 0.8288770318031311, 0.893048107624054, 0.8716577291488647, 0.8770053386688232, 0.8716577291488647, 0.8770053386688232, 0.893048107624054, 0.9144384860992432, 0.893048107624054, 0.893048107624054, 0.8877005577087402, 0.893048107624054, 0.9144384860992432, 0.8983957171440125, 0.893048107624054, 0.9090909361839294, 0.893048107624054, 0.8983957171440125, 0.893048107624054, 0.903743326663971, 0.8983957171440125]
val_loss: [1.3351929187774658, 1.0900132656097412, 0.8609974384307861, 1.1406972408294678, 0.7347314953804016, 1.079147458076477, 0.6718969345092773, 0.5352643132209778, 0.6304271817207336, 0.6675679683685303, 0.5172935128211975, 0.5763021111488342, 0.529177188873291, 0.5813685655593872, 0.4474166929721832, 0.692011296749115, 0.41955310106277466, 0.42367079854011536, 0.6269429922103882, 0.40097010135650635, 0.40183621644973755, 0.5391626954078674, 0.4696868658065796, 0.471340149641037, 0.4086669981479645, 0.3587823510169983, 0.343967467546463, 0.41004112362861633, 0.39838919043540955, 0.4013409912586212, 0.39355623722076416, 0.4056742787361145, 0.3791795074939728, 0.42960941791534424, 0.4414724111557007, 0.5375786423683167, 0.5037510395050049, 0.468545526266098, 0.5538546442985535]

################################################################################################ 

