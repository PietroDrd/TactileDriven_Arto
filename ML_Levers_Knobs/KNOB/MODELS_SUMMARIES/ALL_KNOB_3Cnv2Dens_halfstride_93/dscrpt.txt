def assign_and_deploy_variables_v2(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 5]
        globals()[f"{key}2"] = np.dstack((data[:, :, 0], data[:, :, 4], data[:, :, 2]))
        globals()[f"{key}3"] = np.dstack((data[:, :, 1], data[:, :, 3]))
        
def build_branched_model(input_shape1, input_shape2, input_shape3):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=128, kernel_size=40, strides=20, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = MaxPooling1D(pool_size=2, name='maxpool1d_1_1')(x1)
    x1 = Conv1D(filters=256, kernel_size=8, strides=4, activation='relu', name='conv1d_1_2')(x1)
    x1 = Dropout(rate=0.2, name='dropout_1_1')(x1)
    x1 = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=128, kernel_size=40, strides=20, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = MaxPooling1D(pool_size=2, name='maxpool1d_2_1')(x2)
    x2 = Conv1D(filters=256, kernel_size=8, strides=4, activation='relu', name='conv1d_2_2')(x2)
    x2 = Dropout(rate=0.2, name='dropout_2_1')(x2)
    x2 = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name='conv1d_2_3')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=128, kernel_size=40, strides=20, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = MaxPooling1D(pool_size=2, name='maxpool1d_3_1')(x3)
    x3 = Conv1D(filters=256, kernel_size=8, strides=4, activation='relu', name='conv1d_3_2')(x3)
    x3 = Dropout(rate=0.2, name='dropout_3_1')(x3)
    x3 = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name='conv1d_3_3')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    
    # Concatenate the outputs of the three branches
    merged = concatenate([x1, x2, x3], name='concatenate_1')
    
    # Dense layer
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(6, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3], outputs=output)
    return model

Model: "functional_213"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 3)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 100, 128)  │     15,488 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 100, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 100, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_1_1       │ (None, 50, 128)   │          0 │ conv1d_1_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_2_1       │ (None, 50, 128)   │          0 │ conv1d_2_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_3_1       │ (None, 50, 128)   │          0 │ conv1d_3_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 11, 256)   │    262,400 │ maxpool1d_1_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 11, 256)   │    262,400 │ maxpool1d_2_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 11, 256)   │    262,400 │ maxpool1d_3_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 11, 256)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 11, 256)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 11, 256)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 10, 512)   │    262,656 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 10, 512)   │    262,656 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 10, 512)   │    262,656 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 512)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 512)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 512)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1536)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     98,368 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      1,040 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        102 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 5,117,348 (19.52 MB)
 Trainable params: 1,705,782 (6.51 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 3,411,566 (13.01 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7403f8ea6dd0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.10837350785732269
Test val_loss: 0.7387558221817017
Train accuracy: 0.9681697487831116
Accuracy Score: 0.9285714285714286
F1 Score: 0.9218274397844289
Classification Report:
               precision    recall  f1-score   support

         0.0       0.97      0.97      0.97        31
         1.0       0.96      0.93      0.95        28
         2.0       0.96      0.86      0.91        29
         3.0       1.00      0.92      0.96        13
         4.0       0.93      1.00      0.96        13
         5.0       0.69      0.92      0.79        12

    accuracy                           0.93       126
   macro avg       0.92      0.93      0.92       126
weighted avg       0.94      0.93      0.93       126

Training History:
accuracy: [0.2864721417427063, 0.4031830132007599, 0.44297081232070923, 0.5437665581703186, 0.6206896305084229, 0.6790450811386108, 0.6949602365493774, 0.7347480058670044, 0.8116710782051086, 0.761273205280304, 0.8355437517166138, 0.8488063812255859, 0.8673740029335022, 0.8806366324424744, 0.9018567800521851, 0.912466824054718, 0.9257294535636902, 0.9204244017601013, 0.9389920234680176, 0.9363394975662231, 0.931034505367279, 0.9257294535636902, 0.8938992023468018, 0.912466824054718, 0.9283819794654846, 0.9469496011734009, 0.9496021270751953, 0.9363394975662231, 0.9575597047805786, 0.960212230682373, 0.970822274684906, 0.9681697487831116]
loss: [1.6815561056137085, 1.4418354034423828, 1.3079906702041626, 1.1440882682800293, 0.9767583012580872, 0.8439018726348877, 0.771589994430542, 0.689238429069519, 0.5462231040000916, 0.6019856333732605, 0.44459274411201477, 0.37309354543685913, 0.3550560474395752, 0.29234930872917175, 0.29009923338890076, 0.23935149610042572, 0.18875807523727417, 0.17917507886886597, 0.16608014702796936, 0.20139998197555542, 0.1909433901309967, 0.2304510772228241, 0.2580673396587372, 0.27479925751686096, 0.1786389797925949, 0.1390332132577896, 0.18074986338615417, 0.15744811296463013, 0.1348342001438141, 0.1130935400724411, 0.08112387359142303, 0.10837350785732269]
val_accuracy: [0.3888888955116272, 0.4126984179019928, 0.420634925365448, 0.523809552192688, 0.5714285969734192, 0.6984127163887024, 0.6746031641960144, 0.7222222089767456, 0.6746031641960144, 0.7301587462425232, 0.7936508059501648, 0.7777777910232544, 0.7857142686843872, 0.761904776096344, 0.8253968358039856, 0.7857142686843872, 0.8095238208770752, 0.8253968358039856, 0.7857142686843872, 0.8333333134651184, 0.8253968358039856, 0.8333333134651184, 0.817460298538208, 0.7936508059501648, 0.8492063283920288, 0.8015872836112976, 0.8571428656578064, 0.817460298538208, 0.8333333134651184, 0.8333333134651184, 0.8015872836112976, 0.841269850730896]
val_loss: [1.5774693489074707, 1.4024382829666138, 1.2398810386657715, 1.0918567180633545, 1.038352608680725, 0.833591639995575, 0.8077062964439392, 0.689458429813385, 0.9646346569061279, 0.6779804825782776, 0.5789914131164551, 0.6124230623245239, 0.545754075050354, 0.6125221252441406, 0.5116137862205505, 0.5445126891136169, 0.6271318793296814, 0.5499438047409058, 0.8851215839385986, 0.4844779968261719, 0.587417721748352, 0.6123614311218262, 0.6310687065124512, 0.6996451020240784, 0.6668509840965271, 0.9002825617790222, 0.6247214674949646, 0.5702160000801086, 0.614119291305542, 0.5525156855583191, 0.5885297656059265, 0.7387558221817017]

################################################################################################ 

