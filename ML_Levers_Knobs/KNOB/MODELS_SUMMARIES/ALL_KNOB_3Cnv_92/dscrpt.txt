def assign_and_deploy_variables_v2(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 5]
        globals()[f"{key}2"] = np.dstack((data[:, :, 0], data[:, :, 4], data[:, :, 2]))
        globals()[f"{key}3"] = np.dstack((data[:, :, 1], data[:, :, 3]))

def build_branched_model(input_shape1, input_shape2, input_shape3):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64, kernel_size=40, strides=10, activation='relu', name='conv1d_1_1')(input1)
    x1 = MaxPooling1D(pool_size=2, name='maxpool1d_1_1')(x1)
    x1 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = Dropout(rate=0.1, name='dropout_1_1')(x1)
    x1 = Conv1D(filters=256, kernel_size=4, strides=2, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalAveragePooling1D(name='gap1d_1_1')(x1)
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64, kernel_size=20, strides=8, activation='relu', name='conv1d_2_1')(input2)
    x2 = MaxPooling1D(pool_size=2, name='maxpool1d_2_1')(x2)
    x2 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = Dropout(rate=0.1, name='dropout_2_1')(x2)
    x2 = Conv1D(filters=256, kernel_size=2, strides=2, activation='relu', name='conv1d_2_3')(x2)
    x2 = GlobalAveragePooling1D(name='gap1d_2_1')(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64, kernel_size=40, strides=10, activation='relu', name='conv1d_3_1')(input3)
    x3 = MaxPooling1D(pool_size=2, name='maxpool1d_3_1')(x3)
    x3 = Conv1D(filters=128, kernel_size=10, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = Dropout(rate=0.1, name='dropout_3_1')(x3)
    x3 = Conv1D(filters=256, kernel_size=4, strides=2, activation='relu', name='conv1d_3_3')(x3)
    x3 = GlobalAveragePooling1D(name='gap1d_3_1')(x3)
    
    # Concatenate the outputs of the three branches
    merged = concatenate([x1, x2, x3], name='concatenate_1')
    
    # Dense layer
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    
    # Output layer for 6-class classification
    output = Dense(6, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3], outputs=output)
    return model

Model: "functional_3"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 3)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 197, 64)   │      7,744 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 248, 64)   │      1,344 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 197, 64)   │      5,184 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_1_1       │ (None, 98, 64)    │          0 │ conv1d_1_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_2_1       │ (None, 124, 64)   │          0 │ conv1d_2_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_3_1       │ (None, 98, 64)    │          0 │ conv1d_3_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 46, 128)   │     65,664 │ maxpool1d_1_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 59, 128)   │     65,664 │ maxpool1d_2_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 45, 128)   │     82,048 │ maxpool1d_3_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 46, 128)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 59, 128)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 45, 128)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 22, 256)   │    131,328 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 29, 256)   │     65,792 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 21, 256)   │    131,328 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalAveragePool… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalAveragePool… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalAveragePool… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 768)       │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     49,216 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        390 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,817,108 (6.93 MB)
 Trainable params: 605,702 (2.31 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,211,406 (4.62 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7846ee98e800>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.18130731582641602
Test val_loss: 0.6155971884727478
Train accuracy: 0.9336870312690735
Accuracy Score: 0.9206349206349206
F1 Score: 0.9177275580665412
Classification Report:
               precision    recall  f1-score   support

         0.0       0.97      0.90      0.93        31
         1.0       0.87      0.96      0.92        28
         2.0       1.00      0.86      0.93        29
         3.0       0.93      1.00      0.96        13
         4.0       0.86      0.92      0.89        13
         5.0       0.85      0.92      0.88        12

    accuracy                           0.92       126
   macro avg       0.91      0.93      0.92       126
weighted avg       0.93      0.92      0.92       126

Training History:
accuracy: [0.336870014667511, 0.40848806500434875, 0.44297081232070923, 0.4482758641242981, 0.44297081232070923, 0.48275861144065857, 0.5039787888526917, 0.5305039882659912, 0.5411140322685242, 0.5729442834854126, 0.6366047859191895, 0.6498673558235168, 0.7347480058670044, 0.6790450811386108, 0.6790450811386108, 0.721485435962677, 0.7480106353759766, 0.7506631016731262, 0.7824933528900146, 0.7798408269882202, 0.7877984046936035, 0.790450930595398, 0.8302386999130249, 0.8647214770317078, 0.8461538553237915, 0.8673740029335022, 0.8594164252281189, 0.8435013294219971, 0.8673740029335022, 0.8938992023468018, 0.8779841065406799, 0.912466824054718, 0.9151193499565125, 0.8885941505432129, 0.912466824054718, 0.9204244017601013, 0.9098142981529236, 0.9283819794654846, 0.9230769276618958, 0.9336870312690735]
loss: [1.6608312129974365, 1.444516658782959, 1.3724515438079834, 1.3227734565734863, 1.2970011234283447, 1.217974066734314, 1.1989744901657104, 1.1390080451965332, 1.016560673713684, 0.999706506729126, 0.9304825663566589, 0.9113838076591492, 0.7479817271232605, 0.833246111869812, 0.8059665560722351, 0.7160065770149231, 0.6560444235801697, 0.608643651008606, 0.6103826761245728, 0.5869443416595459, 0.5343573093414307, 0.5287397503852844, 0.4471694529056549, 0.4207509160041809, 0.40774211287498474, 0.342649906873703, 0.3482208549976349, 0.3894147276878357, 0.3305146098136902, 0.31751784682273865, 0.3385511338710785, 0.2522447109222412, 0.24622571468353271, 0.2892862558364868, 0.23991483449935913, 0.23017121851444244, 0.24761979281902313, 0.21334566175937653, 0.1913529634475708, 0.18130731582641602]
val_accuracy: [0.4047619104385376, 0.3888888955116272, 0.4523809552192688, 0.4126984179019928, 0.4365079402923584, 0.420634925365448, 0.4365079402923584, 0.4523809552192688, 0.579365074634552, 0.5952380895614624, 0.5396825671195984, 0.5476190447807312, 0.6507936716079712, 0.6507936716079712, 0.682539701461792, 0.7063491940498352, 0.7222222089767456, 0.6984127163887024, 0.7698412537574768, 0.7142857313156128, 0.7857142686843872, 0.7539682388305664, 0.7222222089767456, 0.7301587462425232, 0.7698412537574768, 0.761904776096344, 0.6746031641960144, 0.7222222089767456, 0.841269850730896, 0.7539682388305664, 0.8015872836112976, 0.817460298538208, 0.6984127163887024, 0.7857142686843872, 0.8015872836112976, 0.8015872836112976, 0.8333333134651184, 0.8333333134651184, 0.8253968358039856, 0.7857142686843872]
val_loss: [1.539318323135376, 1.4455749988555908, 1.3621574640274048, 1.3568260669708252, 1.3028903007507324, 1.2865047454833984, 1.2834796905517578, 1.2638418674468994, 1.0356794595718384, 1.1186654567718506, 1.1200915575027466, 1.0139849185943604, 0.9049500823020935, 0.9157065153121948, 0.8229306936264038, 0.7950817346572876, 0.7101083397865295, 0.7309211492538452, 0.6768208146095276, 0.6882323622703552, 0.6189320683479309, 0.5923421382904053, 0.7144784927368164, 0.6543353199958801, 0.6137799620628357, 0.6049128174781799, 1.0093679428100586, 0.740350604057312, 0.5740482807159424, 0.7379863858222961, 0.6379393339157104, 0.4793078601360321, 0.8006365299224854, 0.5185325145721436, 0.5252894163131714, 0.6672362685203552, 0.4887990653514862, 0.49358657002449036, 0.4847882091999054, 0.6155971884727478]

################################################################################################ 

