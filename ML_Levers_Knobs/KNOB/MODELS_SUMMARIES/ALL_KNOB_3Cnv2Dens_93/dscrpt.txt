def build_branched_model(input_shape1, input_shape2, input_shape3):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=128, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    # x1 = MaxPooling1D(pool_size=2, name='maxpool1d_1_1')(x1)
    x1 = Conv1D(filters=256, kernel_size=8, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = Dropout(rate=0.2, name='dropout_1_1')(x1)
    x1 = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=128, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    # x2 = MaxPooling1D(pool_size=2, name='maxpool1d_2_1')(x2)
    x2 = Conv1D(filters=256, kernel_size=8, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = Dropout(rate=0.2, name='dropout_2_1')(x2)
    x2 = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name='conv1d_2_3')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=128, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    # x3 = MaxPooling1D(pool_size=2, name='maxpool1d_3_1')(x3)
    x3 = Conv1D(filters=256, kernel_size=8, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = Dropout(rate=0.2, name='dropout_3_1')(x3)
    x3 = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name='conv1d_3_3')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    
    # Concatenate the outputs of the three branches
    merged = concatenate([x1, x2, x3], name='concatenate_1')
    
    # Dense layer
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(6, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3], outputs=output)
    return model

Model: "functional_1"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 3)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 200, 128)  │     15,488 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 200, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 200, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 97, 256)   │    262,400 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 97, 256)   │    262,400 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 97, 256)   │    262,400 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 97, 256)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 97, 256)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 97, 256)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 96, 512)   │    262,656 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 96, 512)   │    262,656 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 96, 512)   │    262,656 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 512)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 512)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 512)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1536)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     98,368 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      1,040 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        102 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 5,117,348 (19.52 MB)
 Trainable params: 1,705,782 (6.51 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 3,411,566 (13.01 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x76ab0fc57670>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.24655988812446594
Test val_loss: 0.7890241146087646
Train accuracy: 0.9177718758583069
Accuracy Score: 0.9285714285714286
F1 Score: 0.9303105468385603
Classification Report:
               precision    recall  f1-score   support

         0.0       0.94      0.94      0.94        31
         1.0       0.89      0.89      0.89        28
         2.0       0.96      0.93      0.95        29
         3.0       1.00      0.92      0.96        13
         4.0       1.00      1.00      1.00        13
         5.0       0.79      0.92      0.85        12

    accuracy                           0.93       126
   macro avg       0.93      0.93      0.93       126
weighted avg       0.93      0.93      0.93       126

Training History:
accuracy: [0.2944297194480896, 0.37931033968925476, 0.45358091592788696, 0.45092839002609253, 0.5013262629508972, 0.48806366324424744, 0.5358090400695801, 0.5358090400695801, 0.5596817135810852, 0.5888594388961792, 0.6021220088005066, 0.6074270606040955, 0.6312997341156006, 0.6551724076271057, 0.633952260017395, 0.6525198817253113, 0.7002652287483215, 0.7294429540634155, 0.6870026588439941, 0.7586206793785095, 0.7771883010864258, 0.8196286559104919, 0.7533156275749207, 0.7877984046936035, 0.7639257311820984, 0.7824933528900146, 0.8408488035202026, 0.8673740029335022, 0.8461538553237915, 0.8488063812255859, 0.8594164252281189, 0.883289098739624, 0.8859416246414185, 0.8912466764450073, 0.8885941505432129, 0.9045093059539795, 0.8965517282485962, 0.9018567800521851, 0.883289098739624, 0.9389920234680176, 0.931034505367279, 0.9230769276618958, 0.883289098739624, 0.8726790547370911, 0.8965517282485962, 0.8488063812255859, 0.8726790547370911, 0.8700265288352966, 0.8912466764450073, 0.9257294535636902, 0.9442970752716064, 0.9389920234680176, 0.9575597047805786, 0.9336870312690735, 0.8992042541503906, 0.9257294535636902, 0.8965517282485962, 0.8806366324424744, 0.8992042541503906, 0.9283819794654846, 0.8938992023468018, 0.9204244017601013, 0.9230769276618958, 0.9177718758583069]
loss: [1.700687289237976, 1.5041089057922363, 1.3847072124481201, 1.3075156211853027, 1.261797547340393, 1.2266709804534912, 1.1732200384140015, 1.0965805053710938, 1.0443906784057617, 1.023832082748413, 0.9124240875244141, 0.9440551996231079, 0.8784438967704773, 0.7996692657470703, 0.860317051410675, 0.8124484419822693, 0.7195325493812561, 0.6847254037857056, 0.7470836639404297, 0.6134631633758545, 0.5321952700614929, 0.4891715347766876, 0.5956630110740662, 0.58631432056427, 0.5974328517913818, 0.5383459329605103, 0.4103589355945587, 0.3359132707118988, 0.42665937542915344, 0.370613694190979, 0.3488052189350128, 0.33598777651786804, 0.32906848192214966, 0.2759074568748474, 0.27094513177871704, 0.247653067111969, 0.24620145559310913, 0.25230154395103455, 0.2650177776813507, 0.20845292508602142, 0.177647665143013, 0.20222516357898712, 0.29305920004844666, 0.27980005741119385, 0.2825724482536316, 0.3621833026409149, 0.3387872874736786, 0.35793694853782654, 0.2669326066970825, 0.1721823662519455, 0.1674523651599884, 0.16304509341716766, 0.14570295810699463, 0.15889282524585724, 0.2610110342502594, 0.19764937460422516, 0.2338525652885437, 0.2990565299987793, 0.277860552072525, 0.22862932085990906, 0.28945794701576233, 0.20178192853927612, 0.2184920758008957, 0.24655988812446594]
val_accuracy: [0.3333333432674408, 0.460317462682724, 0.4285714328289032, 0.4047619104385376, 0.4365079402923584, 0.5158730149269104, 0.4444444477558136, 0.5158730149269104, 0.523809552192688, 0.579365074634552, 0.5317460298538208, 0.579365074634552, 0.60317462682724, 0.6428571343421936, 0.5317460298538208, 0.6428571343421936, 0.6428571343421936, 0.6666666865348816, 0.6428571343421936, 0.6984127163887024, 0.738095223903656, 0.7142857313156128, 0.6746031641960144, 0.7063491940498352, 0.60317462682724, 0.7142857313156128, 0.7857142686843872, 0.7142857313156128, 0.7857142686843872, 0.7063491940498352, 0.7857142686843872, 0.8253968358039856, 0.841269850730896, 0.8333333134651184, 0.7142857313156128, 0.7857142686843872, 0.7857142686843872, 0.841269850730896, 0.817460298538208, 0.8650793433189392, 0.8571428656578064, 0.658730149269104, 0.7857142686843872, 0.8571428656578064, 0.7777777910232544, 0.761904776096344, 0.817460298538208, 0.8015872836112976, 0.7936508059501648, 0.841269850730896, 0.817460298538208, 0.8571428656578064, 0.8571428656578064, 0.8333333134651184, 0.738095223903656, 0.8333333134651184, 0.7063491940498352, 0.8333333134651184, 0.8095238208770752, 0.7777777910232544, 0.8492063283920288, 0.8730158805847168, 0.7063491940498352, 0.7301587462425232]
val_loss: [1.6252683401107788, 1.3720310926437378, 1.3662021160125732, 1.2640141248703003, 1.2755303382873535, 1.2019846439361572, 1.2110780477523804, 1.1405324935913086, 1.1021685600280762, 1.0219529867172241, 1.0550143718719482, 0.9931111335754395, 0.9625321626663208, 1.0193754434585571, 0.9938399195671082, 0.9843908548355103, 0.8345305919647217, 0.9128764271736145, 0.9577431082725525, 0.7422326803207397, 0.7324703931808472, 0.6792718768119812, 0.8682901263237, 0.7574889063835144, 0.9242556095123291, 0.7045593857765198, 0.5845730304718018, 0.740989089012146, 0.6646405458450317, 0.8440194725990295, 0.6875038146972656, 0.5584606528282166, 0.5743830800056458, 0.5281753540039062, 0.6738864183425903, 0.5727212429046631, 0.6007920503616333, 0.5288626551628113, 0.5704114437103271, 0.49341854453086853, 0.46115583181381226, 1.029986023902893, 0.5903201699256897, 0.43997499346733093, 0.7360332012176514, 0.6591514945030212, 0.5942021012306213, 0.5058932304382324, 0.5085223317146301, 0.5465366244316101, 0.5397390127182007, 0.42120125889778137, 0.5184543132781982, 0.453238844871521, 0.6562941074371338, 0.5337108373641968, 0.8233672976493835, 0.5618184804916382, 0.648887038230896, 0.7316103577613831, 0.44604459404945374, 0.4796845614910126, 1.2467422485351562, 0.7890241146087646]

################################################################################################ 

