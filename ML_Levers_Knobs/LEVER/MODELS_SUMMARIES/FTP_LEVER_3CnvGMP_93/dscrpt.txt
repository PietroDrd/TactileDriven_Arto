def build_branched_model1(input_shape1, input_shape2, input_shape3):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    # x1 = MaxPooling1D(pool_size=2, name='maxpool1d_1_1')(x1)
    x1 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = Dropout(rate=0.2, name='dropout_1_1')(x1)
    x1 = Conv1D(filters=256, kernel_size=2, strides=1, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    # x2 = MaxPooling1D(pool_size=2, name='maxpool1d_2_1')(x2)
    x2 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = Dropout(rate=0.2, name='dropout_2_1')(x2)
    x2 = Conv1D(filters=128, kernel_size=2, strides=1, activation='relu', name='conv1d_2_3')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    # x3 = MaxPooling1D(pool_size=2, name='maxpool1d_3_1')(x3)
    x3 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = Dropout(rate=0.2, name='dropout_3_1')(x3)
    x3 = Conv1D(filters=128, kernel_size=2, strides=1, activation='relu', name='conv1d_3_3')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    
    # Concatenate the outputs of the three branches
    merged = concatenate([x1, x2, x3], name='concatenate_1')
    
    # Dense layer
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3], outputs=output)
    return model

Model: "functional_1"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 200, 64)   │      5,184 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 200, 64)   │      2,624 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 200, 64)   │      2,624 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 97, 128)   │     65,664 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 97, 128)   │     65,664 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 97, 128)   │     65,664 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 97, 128)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 97, 128)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 97, 128)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 96, 256)   │     65,792 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 96, 128)   │     32,896 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 96, 128)   │     32,896 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 128)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 128)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 512)       │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     32,832 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      1,040 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 3)         │         51 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,118,795 (4.27 MB)
 Trainable params: 372,931 (1.42 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 745,864 (2.85 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x783cd8e77520>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.04569463059306145
Test val_loss: 0.15840278565883636
Train accuracy: 0.9813277721405029
Accuracy Score: 0.9254658385093167
F1 Score: 0.9180556792497091
Classification Report:
               precision    recall  f1-score   support

         0.0       0.96      0.98      0.97        66
         1.0       0.96      0.85      0.90        59
         2.0       0.83      0.94      0.88        36

    accuracy                           0.93       161
   macro avg       0.92      0.93      0.92       161
weighted avg       0.93      0.93      0.93       161

Training History:
accuracy: [0.37136930227279663, 0.44605809450149536, 0.44398340582847595, 0.4834024906158447, 0.46680498123168945, 0.4979253113269806, 0.48547717928886414, 0.5331950187683105, 0.5103734731674194, 0.5705394148826599, 0.5601660013198853, 0.6390041708946228, 0.5539419054985046, 0.6203319430351257, 0.680497944355011, 0.6721991896629333, 0.6701244711875916, 0.6390041708946228, 0.6887966990470886, 0.7074688673019409, 0.7551867365837097, 0.7074688673019409, 0.7883817553520203, 0.7987551689147949, 0.8215767741203308, 0.865145206451416, 0.865145206451416, 0.8879668116569519, 0.8921161890029907, 0.910788357257843, 0.8879668116569519, 0.9211618304252625, 0.8796680569648743, 0.9170124530792236, 0.9024896025657654, 0.8879668116569519, 0.9211618304252625, 0.9585062265396118, 0.9605808854103088, 0.9377593398094177, 0.9502074718475342, 0.9647302627563477, 0.9502074718475342, 0.95643150806427, 0.9668049812316895, 0.9585062265396118, 0.910788357257843, 0.9481327533721924, 0.9688796401023865, 0.9688796401023865, 0.9730290174484253, 0.9730290174484253, 0.9792531132698059, 0.9730290174484253, 0.9668049812316895, 0.9813277721405029, 0.9896265268325806, 0.9792531132698059, 0.9626556038856506, 0.9668049812316895, 0.9834024906158447, 0.9896265268325806, 0.9792531132698059, 0.9771783947944641, 0.9834024906158447, 0.9854771494865417, 0.9813277721405029, 0.9854771494865417, 0.9875518679618835, 0.9917012453079224, 0.9813277721405029, 0.9688796401023865, 0.9709543585777283, 0.9792531132698059, 0.9647302627563477, 0.9149377346038818, 0.954356849193573, 0.9792531132698059, 0.9834024906158447, 0.9813277721405029, 0.9813277721405029, 0.9688796401023865, 0.9875518679618835, 0.9792531132698059, 0.9834024906158447, 0.9917012453079224, 0.9875518679618835, 0.9875518679618835, 0.9917012453079224, 0.9813277721405029]
loss: [1.0789804458618164, 1.0300887823104858, 1.023851990699768, 1.021843671798706, 0.9950649738311768, 0.968218207359314, 0.9642744660377502, 0.9323311448097229, 0.9402161836624146, 0.8890233039855957, 0.8395057320594788, 0.7434651255607605, 0.9662908315658569, 0.7849253416061401, 0.7089637517929077, 0.6634405255317688, 0.6149722933769226, 0.7770594954490662, 0.6392837166786194, 0.6158832907676697, 0.5511437058448792, 0.5712741613388062, 0.496610164642334, 0.45720374584198, 0.41878339648246765, 0.3536636531352997, 0.3306514322757721, 0.2987222671508789, 0.26329222321510315, 0.2337130606174469, 0.28528308868408203, 0.2147710770368576, 0.28073650598526, 0.22962597012519836, 0.2551015615463257, 0.31380346417427063, 0.2638937830924988, 0.20363761484622955, 0.16197338700294495, 0.18797621130943298, 0.15028522908687592, 0.10983934998512268, 0.14284484088420868, 0.12177418917417526, 0.0918833464384079, 0.1095505878329277, 0.2604289650917053, 0.13611753284931183, 0.10837200284004211, 0.09962128102779388, 0.08599518984556198, 0.09096483886241913, 0.06044522300362587, 0.08080072700977325, 0.08160704374313354, 0.05651908740401268, 0.04510311782360077, 0.06580287218093872, 0.09356202185153961, 0.08023098856210709, 0.057496268302202225, 0.040559012442827225, 0.0571177676320076, 0.05592217296361923, 0.0657692700624466, 0.058421436697244644, 0.04797271639108658, 0.04472624510526657, 0.04270050674676895, 0.03715753182768822, 0.056013260036706924, 0.08519018441438675, 0.08216077089309692, 0.078241266310215, 0.08064862340688705, 0.25375744700431824, 0.1192629337310791, 0.08148583024740219, 0.05931921303272247, 0.05063118785619736, 0.062009405344724655, 0.08881310373544693, 0.043237749487161636, 0.07506681233644485, 0.056133292615413666, 0.04344836249947548, 0.03027682565152645, 0.03346846625208855, 0.03599894419312477, 0.04569463059306145]
val_accuracy: [0.4161490797996521, 0.39751553535461426, 0.4161490797996521, 0.40993788838386536, 0.42236024141311646, 0.4534161388874054, 0.4037266969680786, 0.47204968333244324, 0.4534161388874054, 0.5031055808067322, 0.5527950525283813, 0.5341615080833435, 0.5279502868652344, 0.5776397585868835, 0.6459627151489258, 0.6832298040390015, 0.5714285969734192, 0.7080745100975037, 0.7018633484840393, 0.695652186870575, 0.6770186424255371, 0.7888198494911194, 0.8074533939361572, 0.7888198494911194, 0.7701863646507263, 0.9006211161613464, 0.9068322777748108, 0.8819875717163086, 0.9254658222198486, 0.9006211161613464, 0.9254658222198486, 0.9130434989929199, 0.9316770434379578, 0.888198733329773, 0.9254658222198486, 0.9192546606063843, 0.9006211161613464, 0.9378882050514221, 0.95652174949646, 0.95652174949646, 0.9440993666648865, 0.9627329111099243, 0.9192546606063843, 0.9378882050514221, 0.9440993666648865, 0.9503105878829956, 0.9316770434379578, 0.95652174949646, 0.9627329111099243, 0.9378882050514221, 0.9068322777748108, 0.9316770434379578, 0.9627329111099243, 0.95652174949646, 0.9378882050514221, 0.9192546606063843, 0.9503105878829956, 0.95652174949646, 0.95652174949646, 0.95652174949646, 0.9440993666648865, 0.9378882050514221, 0.9627329111099243, 0.9130434989929199, 0.9503105878829956, 0.9689440727233887, 0.9503105878829956, 0.9627329111099243, 0.95652174949646, 0.9689440727233887, 0.9503105878829956, 0.9130434989929199, 0.9689440727233887, 0.9192546606063843, 0.9254658222198486, 0.9440993666648865, 0.9751552939414978, 0.9627329111099243, 0.9689440727233887, 0.9440993666648865, 0.9751552939414978, 0.9689440727233887, 0.95652174949646, 0.9627329111099243, 0.9627329111099243, 0.9751552939414978, 0.9378882050514221, 0.9316770434379578, 0.9068322777748108, 0.9440993666648865]
val_loss: [1.0667001008987427, 1.0415592193603516, 1.1264954805374146, 1.055734634399414, 1.0579322576522827, 1.057562232017517, 0.9996733069419861, 0.9924201965332031, 1.101864218711853, 1.002039909362793, 0.8742231130599976, 1.0021306276321411, 0.9303792715072632, 0.8421600461006165, 0.7851199507713318, 0.7720966935157776, 0.9051451086997986, 0.7798424959182739, 0.7812855839729309, 0.688373863697052, 0.6869657039642334, 0.6450996398925781, 0.5618416666984558, 0.5993353128433228, 0.46145913004875183, 0.36836764216423035, 0.35326677560806274, 0.33369961380958557, 0.30188602209091187, 0.3479721248149872, 0.2742774784564972, 0.25458139181137085, 0.30358290672302246, 0.3335341811180115, 0.35064032673835754, 0.3789809048175812, 0.3369498550891876, 0.24500663578510284, 0.1971650868654251, 0.1772942990064621, 0.18538394570350647, 0.15606491267681122, 0.24069225788116455, 0.18519863486289978, 0.1669074445962906, 0.17379111051559448, 0.18656885623931885, 0.16096338629722595, 0.1578982025384903, 0.18462397158145905, 0.26386117935180664, 0.19334852695465088, 0.15545380115509033, 0.18947593867778778, 0.19849075376987457, 0.24139060080051422, 0.18555448949337006, 0.21258778870105743, 0.15011493861675262, 0.1548602432012558, 0.20891647040843964, 0.24235674738883972, 0.17900878190994263, 0.2869725525379181, 0.20196940004825592, 0.16266143321990967, 0.17253966629505157, 0.13149131834506989, 0.1650746464729309, 0.1494579315185547, 0.17513620853424072, 0.3137480318546295, 0.1363062709569931, 0.29113006591796875, 0.23274147510528564, 0.19672274589538574, 0.12836124002933502, 0.14086587727069855, 0.1173480972647667, 0.2061900794506073, 0.12373671680688858, 0.28188472986221313, 0.14544552564620972, 0.1409909874200821, 0.13656216859817505, 0.11285815387964249, 0.1747143417596817, 0.16693444550037384, 0.2618790864944458, 0.15840278565883636]

################################################################################################ 

