def build_branched_model3(input_shape1, input_shape2, input_shape3):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=128, kernel_size=100, strides=20, activation='relu', padding='same', name='conv1d_1_0')(input1)
    x1 = Conv1D(filters=128, kernel_size=20, strides=8, activation='relu', name='conv1d_1_1')(x1)
    x1 = Conv1D(filters=256, kernel_size=8, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = Dropout(rate=0.3, name='dropout_1_1')(x1)
    x1 = Conv1D(filters=512, kernel_size=2, strides=1, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)

    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64, kernel_size=100, strides=20, activation='relu', padding='same', name='conv1d_2_0')(input2)
    x2 = Conv1D(filters=128, kernel_size=20, strides=8, activation='relu', name='conv1d_2_1')(x2)
    x2 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = Dropout(rate=0.3, name='dropout_2_1')(x2)
    x2 = Conv1D(filters=128, kernel_size=2, strides=1, activation='relu', name='conv1d_2_3')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)

    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64, kernel_size=100, strides=20, activation='relu', padding='same', name='conv1d_3_0')(input3)
    x3 = Conv1D(filters=128, kernel_size=20, strides=8, activation='relu', name='conv1d_3_1')(x3)
    x3 = Conv1D(filters=128, kernel_size=8, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = Dropout(rate=0.3, name='dropout_3_1')(x3)
    x3 = Conv1D(filters=128, kernel_size=2, strides=1, activation='relu', name='conv1d_3_3')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)

    # Concatenate the outputs of the three branches
    merged = concatenate([x1, x2, x3], name='concatenate_1')

    # Dense layers
    dense = Dense(128, activation='relu', name='dense_1')(merged)
    dense = Dense(16, activation='relu', name='dense_2')(dense)

    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)

    # Create the model
    model = Model(inputs=[input1, input2, input3], outputs=output)

    return model

Model: "functional_7"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_0 (Conv1D) │ (None, 100, 128)  │     25,728 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_0 (Conv1D) │ (None, 100, 64)   │      6,464 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_0 (Conv1D) │ (None, 100, 64)   │      6,464 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 11, 128)   │    327,808 │ conv1d_1_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 11, 128)   │    163,968 │ conv1d_2_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 11, 128)   │    163,968 │ conv1d_3_0[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 2, 256)    │    262,400 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 2, 128)    │    131,200 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 2, 128)    │    131,200 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 2, 256)    │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 2, 128)    │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 2, 128)    │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 1, 512)    │    262,656 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 1, 128)    │     32,896 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 1, 128)    │     32,896 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 512)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 128)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 128)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 768)       │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 128)       │     98,432 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      2,064 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 3)         │         51 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 4,944,587 (18.86 MB)
 Trainable params: 1,648,195 (6.29 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 3,296,392 (12.57 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x75a01c43e440>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.05397409573197365
Test val_loss: 0.3530656695365906
Train accuracy: 0.9875518679618835
Accuracy Score: 0.968944099378882
F1 Score: 0.9657783179501632
Classification Report:
               precision    recall  f1-score   support

         0.0       0.99      0.99      0.99        72
         1.0       0.94      0.96      0.95        47
         2.0       0.98      0.95      0.96        42

    accuracy                           0.97       161
   macro avg       0.97      0.97      0.97       161
weighted avg       0.97      0.97      0.97       161

Training History:
accuracy: [0.42116183042526245, 0.5518672466278076, 0.6161825656890869, 0.7448132634162903, 0.8091286420822144, 0.8983402252197266, 0.9149377346038818, 0.9377593398094177, 0.9273858666419983, 0.9522821307182312, 0.9585062265396118, 0.9481327533721924, 0.9626556038856506, 0.95643150806427, 0.9668049812316895, 0.9792531132698059, 0.9688796401023865, 0.9688796401023865, 0.9688796401023865, 0.9813277721405029, 0.9730290174484253, 0.9875518679618835, 0.9771783947944641, 0.9688796401023865, 0.9751037359237671, 0.9792531132698059, 0.9751037359237671, 0.9813277721405029, 0.9854771494865417, 0.9813277721405029, 0.9834024906158447, 0.9834024906158447, 0.9875518679618835]
loss: [1.0547503232955933, 0.9393669366836548, 0.8007699251174927, 0.5830485224723816, 0.5213249325752258, 0.3491613566875458, 0.2563912272453308, 0.19264596700668335, 0.20419105887413025, 0.1527395099401474, 0.175970658659935, 0.2403971254825592, 0.1505354344844818, 0.1443643569946289, 0.10935799777507782, 0.08755408972501755, 0.11028053611516953, 0.14294315874576569, 0.11637502163648605, 0.10823100805282593, 0.09581942111253738, 0.05904911085963249, 0.10124050080776215, 0.10354243218898773, 0.07981612533330917, 0.06950037181377411, 0.08841060101985931, 0.07172144949436188, 0.04832134395837784, 0.05425889790058136, 0.07957043498754501, 0.06209766864776611, 0.05397409573197365]
val_accuracy: [0.590062141418457, 0.5776397585868835, 0.7142857313156128, 0.8012422323226929, 0.8571428656578064, 0.8819875717163086, 0.9130434989929199, 0.9130434989929199, 0.9130434989929199, 0.9130434989929199, 0.9316770434379578, 0.9254658222198486, 0.9378882050514221, 0.9440993666648865, 0.9378882050514221, 0.9378882050514221, 0.9316770434379578, 0.9192546606063843, 0.9316770434379578, 0.9503105878829956, 0.9440993666648865, 0.9503105878829956, 0.9254658222198486, 0.9440993666648865, 0.9440993666648865, 0.9503105878829956, 0.9440993666648865, 0.9440993666648865, 0.9440993666648865, 0.9503105878829956, 0.9440993666648865, 0.9378882050514221, 0.9503105878829956]
val_loss: [0.9599025845527649, 0.9008222222328186, 0.6777383685112, 0.5443869829177856, 0.49725645780563354, 0.42905303835868835, 0.39312925934791565, 0.4289821982383728, 0.37356191873550415, 0.5037217140197754, 0.44321712851524353, 0.4663761258125305, 0.33656853437423706, 0.3456757366657257, 0.44233420491218567, 0.4863717555999756, 0.3862714469432831, 0.4625311493873596, 0.3757420480251312, 0.28353434801101685, 0.25077950954437256, 0.30223584175109863, 0.4863382577896118, 0.3440091907978058, 0.37647104263305664, 0.41441258788108826, 0.3458797037601471, 0.45195338129997253, 0.3971031904220581, 0.5093095898628235, 0.3505013883113861, 0.43145695328712463, 0.3530656695365906]

################################################################################################ 

